{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0917ed-87a5-4602-be73-3cb6fba0d129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import pathlib\n",
    "import fiona\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from geopandas.tools import geocode\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fadcf16a-c025-4089-9669-28392b5de6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "BELLINZONA_UST = 2101\n",
    "BLENIO_UST = 2102\n",
    "LEVENTINA_UST = 2103\n",
    "LOCARNO_UST = 2104\n",
    "LUGANO_UST = 2105\n",
    "MENDRISIO_UST = 2106\n",
    "RIVIERA_UST = 2107\n",
    "VALLEMAGGIA = 2108\n",
    "\n",
    "### ODIS ### \n",
    "# AVG_RES_LIGHT_DENSITY = 8.3 # kWh/m2y ODIS\n",
    "# AVG_RES_APP =  1400 # kWh/y ODIS, \n",
    "\n",
    "### ROHDATEN Tarife 2020 ELCOM ###\n",
    "# H1 - 1600 kWh/an: logement de 2 pièces avec cuisinière électrique\n",
    "# H2 - 2500 kWh/an: logement de 4 pièces avec cuisinière électrique; \n",
    "# H4 - 4500 kWh/an: logement de 5 pièces avec cuisinière électrique et sèche-linge (sans chauffe-eau électrique) \n",
    "# H8 - 7500 kWh/an: grand logement en propriété, avec large utilisation de l'électricité \n",
    "# C1 - 8000 kWh/an: très petite entreprise, puissance max.: 8 kW \n",
    "# C2 - 30 000 kWh/an: petite entreprise, puissance max.: 15 kW \n",
    "# C3 - 150 000 kWh/an: entreprise moyenne, puissance max.: 50 kW\n",
    "# C4 - 500 000 kWh/an: grande entreprise, puissance max.: 150 kW, courant basse tension / C5 - courant moyenne tension, propre station de transformation \n",
    "# C6 - 1 500 000 kWh/an: grande entreprise, puissance max.: 400 kW, courant moyenne tension, propre station de transformation \n",
    "# C7 - 7 500 000 kWh/an: grande entreprise, puissance max.: 1630 kW, courant moyenne tension, propre station de transformation \n",
    "\n",
    "### Consumi di energia (in gigawattora), secondo la destinazione e il vettore energetico, in Ticino, nel 2020\n",
    "S1_AGR_elec = 64000 # kWh/y\n",
    "S2_IND_elec = 160000 #kWh/y\n",
    "S3_SER_elec = 25000 #kWh/y\n",
    "\n",
    "### https://pubdb.bfe.admin.ch/it/publication/download/10559 ###\n",
    "# MFH_2pp_elec = 2190 # kWh/y =  apps and light\n",
    "# MFH_1pp_elec_diff = 458.5 # kWh/y =  apps and light\n",
    "# SFH_4pp_elec = 4048 # kWh/y =  apps and light\n",
    "# SFH_1pp_elec_diff = 593.5 # kWh/y =  apps and light\n",
    "# --> MFH 2PP = 2190\n",
    "# --> SHF 3PP = 3454.5\n",
    "\n",
    "AVG_DHW_PP = 40 # L/DAY/PERSON ODIS\n",
    "AVG_M2_PP = 49.1 #TI - Superficie media per occupante, per Cantone, 2021, https://www.bfs.admin.ch/bfs/it/home/statistiche/costruzioni-abitazioni/abitazioni/condizioni-abitazione/superficie-persona.html\n",
    "SUPERFICIE_MEDIA = 97.8 #https://www.bfs.admin.ch/bfs/it/home/statistiche/costruzioni-abitazioni/abitazioni/dimensioni.html\n",
    "\n",
    "MFH_PP = 2.1 #https://www.bfs.admin.ch/bfs/it/home/statistiche/costruzioni-abitazioni/abitazioni/condizioni-abitazione/densita-utilizzazione.html\n",
    "SFH_PP = 2.7\n",
    "\n",
    "GKLAS = [1110,1121,1122,1130, 1211, 1212, 1220, 1230, 1251, 1261, 1262, 1263, 1264, 1265, 1272, 1241, 1242, 1271, 1274, 1275, 1276, 1277, 1278, 1231, 1252, 1273]\n",
    "\n",
    "GKLAS_SFH = 1110\n",
    "GKLAS_DFH = 1121\n",
    "GKLAS_MFH = 1122\n",
    "GKLAS_RES = [GKLAS_SFH, GKLAS_DFH, GKLAS_MFH]\n",
    "\n",
    "GKLAS_HTG = [1110, 1121, 1122, 1130, 1211, 1212, 1220, 1230, 1251, 1261, 1262, 1263, 1264, 1265, 1272, 1231]\n",
    "GKLAS_NOHTG = [1241, 1242, 1252, 1271, 1273, 1274, 1275, 1276, 1277, 1278]\n",
    "\n",
    "GBAUP = [8011,8012,8013,8014,8015,8016,8017,8018,8019,8020,8021,8022,8023]\n",
    "GSTAT = 1004 # existing\n",
    "   \n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "\n",
    "#SELECT CANTON\n",
    "#CANTON = \"Genève\"\n",
    "CANTON = \"Ticino\"\n",
    "\n",
    "# Read the shp file and decode the Geopandas dataframe using the Swiss coordinates (epsg code: 2056)\n",
    "SWISSTOPO_DISTRICT_FILE = \"swissboundaries3d_2023-01_2056_5728.shp/swissBOUNDARIES3D_1_4_TLM_BEZIRKSGEBIET.shp\" #shapefile downloaded from https://www.swisstopo.admin.ch/fr/geodata/landscape/boundaries3d.html\n",
    "SWISSTOPO_CANTON_FILE = \"swissboundaries3d_2023-01_2056_5728.shp/swissBOUNDARIES3D_1_4_TLM_KANTONSGEBIET.shp\" #shapefile downloaded from https://www.swisstopo.admin.ch/fr/geodata/landscape/boundaries3d.html. NOTE: The actual shapefile (.shp) is useless without the companion files: .dbf, .shx, .prj etc..\n",
    "### SWISSTOPO_CANTON_FILE = figureout how to get the municipal boundaries https://api3.geo.admin.ch/rest/services/api/MapServer/ch.swisstopo.swissboundaries3d-gemeinde-flaeche.fill\n",
    "\n",
    "CANTON_FILE_PATH = MAP_DIRECTORY / SWISSTOPO_CANTON_FILE\n",
    "DISTRICT_FILE_PATH = MAP_DIRECTORY / SWISSTOPO_DISTRICT_FILE\n",
    "\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9fa347-507d-4624-b306-e57c0f8c6b1a",
   "metadata": {},
   "source": [
    "### AGR, IND, SER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8c6c1e-116b-43f6-bf4c-71b8e564c456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bellinzona</th>\n",
       "      <th>Blenio</th>\n",
       "      <th>Leventina</th>\n",
       "      <th>Locarno</th>\n",
       "      <th>Lugano</th>\n",
       "      <th>Mendrisio</th>\n",
       "      <th>Riviera</th>\n",
       "      <th>Vallemaggia</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1_AGR</th>\n",
       "      <td>10725360</td>\n",
       "      <td>7643360</td>\n",
       "      <td>7088600</td>\n",
       "      <td>12081440</td>\n",
       "      <td>15718200</td>\n",
       "      <td>11465040</td>\n",
       "      <td>2835440</td>\n",
       "      <td>4438080</td>\n",
       "      <td>71995520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2_IND</th>\n",
       "      <td>94494750</td>\n",
       "      <td>16286900</td>\n",
       "      <td>23047500</td>\n",
       "      <td>155032850</td>\n",
       "      <td>348478200</td>\n",
       "      <td>156569350</td>\n",
       "      <td>25659550</td>\n",
       "      <td>24430350</td>\n",
       "      <td>843999450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S3_SER</th>\n",
       "      <td>86080580</td>\n",
       "      <td>7119780</td>\n",
       "      <td>11666680</td>\n",
       "      <td>118707360</td>\n",
       "      <td>362421200</td>\n",
       "      <td>123054640</td>\n",
       "      <td>12709140</td>\n",
       "      <td>8716740</td>\n",
       "      <td>730476120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Bellinzona    Blenio  Leventina    Locarno     Lugano  Mendrisio   \n",
       "S1_AGR    10725360   7643360    7088600   12081440   15718200   11465040  \\\n",
       "S2_IND    94494750  16286900   23047500  155032850  348478200  156569350   \n",
       "S3_SER    86080580   7119780   11666680  118707360  362421200  123054640   \n",
       "\n",
       "         Riviera  Vallemaggia      Total  \n",
       "S1_AGR   2835440      4438080   71995520  \n",
       "S2_IND  25659550     24430350  843999450  \n",
       "S3_SER  12709140      8716740  730476120  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the electric demand for agriculture, industry, and services - top down - CODE WORKS\n",
    "\n",
    "### Consumi di energia (in gigawattora), secondo la destinazione e il vettore energetico, in Ticino, nel 2020\n",
    "S1_AGR_elec = 61640 # kWh/y = consumo \"Altro (cantieri, agricoltura ecc.)\" 2020 / Aziende primario 2020\n",
    "S2_IND_elec = 153650 #kWh/y consumo \"Artigianato e industria\"/ Aziende secondario 2020\n",
    "S3_SER_elec = 22180 #kWh/y consumo \"Commercio e servizi\" /Aziende terzario 2020\n",
    "\n",
    "fileDir = DATA_DIRECTORY / \"support_data\" \n",
    "data_s123 = pd.read_csv(fileDir/\"S1_S2_S3.csv\" , header=0, index_col=0)\n",
    "\n",
    "S123 = pd.DataFrame()\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "for dis_num in UST_district:\n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    dis_S1_tot_elec = data_s123.loc[DISTRICT, \"S1_Aziende\"]*S1_AGR_elec\n",
    "    dis_S2_tot_elec = data_s123.loc[DISTRICT, \"S2_Aziende\"]*S2_IND_elec\n",
    "    dis_S3_tot_elec = data_s123.loc[DISTRICT, \"S3_Aziende\"]*S3_SER_elec\n",
    "    \n",
    "    tot_elec = pd.DataFrame(np.array([[dis_S1_tot_elec], [dis_S2_tot_elec], [dis_S3_tot_elec]]),  columns = [DISTRICT], index = [ \"S1_AGR\", \"S2_IND\", \"S3_SER\"]) \n",
    "    \n",
    "    S123 = pd.concat([S123,tot_elec], axis = 1) #,ignore_index=True,\n",
    "S123[\"Total\"] = S123.sum(axis=1)\n",
    "S123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528011c6-ea75-4da1-a175-e22001f62521",
   "metadata": {},
   "source": [
    "### RESIDENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c2c62f-c11b-4918-96e3-96d5292e7178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hh_MFH():\n",
    "\n",
    "    pathlib.Path()\n",
    "    NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "    p = NOTEBOOK_PATH.parent\n",
    "    DATA_DIRECTORY = p / \"data\"\n",
    "    MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "    BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "    CANTON = \"Ticino\"\n",
    "    CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "    REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "           'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "           'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "           'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "           'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "           'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "    UST_file = \"UST_comune.csv\"\n",
    "    canton =\"TI\"\n",
    "    UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "    UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "    UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    TI_hh = pd.DataFrame()\n",
    "    district_hh = pd.DataFrame()\n",
    "    ewid_count = pd.DataFrame()\n",
    "\n",
    "\n",
    "    AVG_HH_ELEC_TI = 2741 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "    UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "    #dis_num = BELLINZONA_UST\n",
    "    #com_num = 5001\n",
    "\n",
    "    for dis_num in UST_district:\n",
    "        communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]\n",
    "\n",
    "        for com_num in communes:\n",
    "            #print(\"comune: \", com_num)\n",
    "\n",
    "            fileloc =  CANTON_CSV_DIRECTORY\n",
    "\n",
    "            filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "            filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "            if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "                data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "                #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "            elif pathlib.Path(fileloc/filename_data).exists():\n",
    "                data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "                #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "            data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "            data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "            data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "            data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "            data_res = data_res.reset_index(drop=True)\n",
    "\n",
    "            for g in range(len(data_res)):\n",
    "                 if (np.isnan(data_res.at[g,\"gklas\"])):\n",
    "                    if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                        data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "                    elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                        data_res.loc[g,\"gklas_fill\"] = 1122\n",
    "\n",
    "            data_res[\"egid_duplicate\"] = data_res.duplicated(subset = [\"egid\"], keep = False)\n",
    "\n",
    "            no_nan = data_res.dropna(subset = [\"ewid\"])\n",
    "            no_nan = no_nan.reset_index(drop=True) #,, inplace=True\n",
    "\n",
    "            for g in range(len(no_nan)):\n",
    "                ewid_count.at[g,\"ewid\"] = ast.literal_eval(no_nan.at[g,\"ewid\"]) \n",
    "\n",
    "            no_nan[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "            duplicates = no_nan.loc[no_nan[\"egid_duplicate\"] == True]\n",
    "            group = duplicates.groupby(\"egid\")[\"ewid_len\"].sum()\n",
    "            group = pd.DataFrame(group)\n",
    "\n",
    "            # Replace ewid_len values in DataFrame A based on DataFrame B\n",
    "            no_nan['ewid_len'] = no_nan.apply(lambda row: group.loc[row['egid'], 'ewid_len'] if row['egid'] in group.index else row['ewid_len'], axis=1)\n",
    "            no_nan.drop_duplicates(subset =[\"egid\"], keep=\"first\", inplace = True)\n",
    "\n",
    "            district_hh = pd.concat([district_hh,no_nan])\n",
    "\n",
    "        abi_num = district_hh [[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "        abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "        abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "        abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "        med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "        med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "        MFH = med.loc[1122.0]\n",
    "        MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "\n",
    "        DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "        hh = pd.DataFrame([MFH_avghh], index=[\"MFH_avg_households\"], columns=[DISTRICT])\n",
    "\n",
    "        TI_hh = pd.concat([TI_hh,hh], axis=1)\n",
    "        district_hh = district_hh[0:0]\n",
    "\n",
    "    TI_hh[\"TI\"] = TI_hh.mean(axis=1)\n",
    "    return TI_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7aaf02b-2376-455a-b660-2089245d69b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_number_list(num):\n",
    "    number_list = [str(i) for i in range(1, num + 1)]\n",
    "    return number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c2fd6e-9417-4033-a911-a8935f1959b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hh_stats():\n",
    "    pathlib.Path()\n",
    "    NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "    p = NOTEBOOK_PATH.parent\n",
    "    DATA_DIRECTORY = p / \"data\"\n",
    "    MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "    BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "\n",
    "    #SELECT CANTON\n",
    "    #CANTON = \"Genève\"\n",
    "    CANTON = \"Ticino\"\n",
    "\n",
    "    CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "\n",
    "    UST_file = \"UST_comune.csv\"\n",
    "    canton =\"TI\"\n",
    "    UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "    UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "    UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    sup_file = DATA_DIRECTORY / \"support_data\" \n",
    "\n",
    "    data_households_2020 = pd.read_excel(sup_file/\"household_inventory_2020-10_2056.xlsx\", sheet_name=\"ZWG_2020_Q3\" , header=0, index_col=0)\n",
    "    data_households_2023 = pd.read_excel(sup_file/\"household_inventory_2023-03_2056.xlsx\", sheet_name=\"ZWG_2023\" , header=0, index_col=0)\n",
    "    \n",
    "    ## GET ratios for primary households and difference between 2023 and 2020 - WORKS!\n",
    "\n",
    "    TI_hh_2020 = pd.DataFrame()\n",
    "    district_hh_2020 = pd.DataFrame()\n",
    "    district_hh_2023 = pd.DataFrame()\n",
    "\n",
    "    for dis_num in UST_district:\n",
    "        communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"]    \n",
    "        DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "\n",
    "        for com_name in communes:\n",
    "            hh_2020 = data_households_2020.loc[data_households_2020[\"Name\"] == com_name]#\n",
    "            hh_2020 = hh_2020.reset_index()\n",
    "\n",
    "            hh_2023 = data_households_2023.loc[data_households_2023[\"Name\"] == com_name]#\n",
    "            hh_2023 = hh_2023.reset_index()\n",
    "\n",
    "            district_hh_2020 = pd.concat([district_hh_2020, hh_2020])\n",
    "            district_hh_2023 = pd.concat([district_hh_2023, hh_2023])\n",
    "\n",
    "        dis_tot_hh_2020 = district_hh_2020[\"ZWG_3150\"].sum()\n",
    "        dis_tot_primary_2020 = district_hh_2020[\"ZWG_3010\"].sum() + district_hh_2020[\"ZWG_3100\"].sum()\n",
    "\n",
    "        calc_dist_secondary_2020 = round(((dis_tot_hh_2020 - dis_tot_primary_2020)/dis_tot_hh_2020),3)\n",
    "\n",
    "        ratio_secondary_mean_2020 = district_hh_2020[\"ZWG_3120\"].mean()\n",
    "        ratio_secondary_median_2020 = district_hh_2020[\"ZWG_3120\"].median()\n",
    "\n",
    "        dis_tot_hh_2023 = district_hh_2023[\"ZWG_3150\"].sum()\n",
    "        dis_tot_primary_2023 = district_hh_2023[\"ZWG_3010\"].sum() + district_hh_2023[\"ZWG_3100\"].sum()\n",
    "\n",
    "        hh_diff = round((dis_tot_hh_2020/dis_tot_hh_2023),3)\n",
    "\n",
    "        d_hh = pd.DataFrame([calc_dist_secondary_2020, hh_diff], index=[\"hh2020_p_ratio\", \"hh2023_diff\"], columns=[DISTRICT])\n",
    "\n",
    "        TI_hh_2020 = pd.concat([TI_hh_2020,d_hh], axis=1)\n",
    "        district_hh_2020 = district_hh_2020[0:0]\n",
    "        district_hh_2023 = district_hh_2023[0:0]\n",
    "\n",
    "    #print(\"calc = \", calc_dist_secondary_2020, \"mean = \", ratio_secondary_mean_2020, \"median = \", ratio_secondary_median_2020)\n",
    "    #print(\"tot_hh_2020 = \",dis_tot_hh_2020 ,\"primary_hh_2020 = \",dis_tot_primary_2020, \"ratio = \", dis_tot_primary_2020/dis_tot_hh_2020 )\n",
    "    #print(\"tot_hh_2023 = \",dis_tot_hh_2023 ,\"primary_hh_2023 = \",dis_tot_primary_2023, \"ratio = \", dis_tot_primary_2023/dis_tot_hh_2023)\n",
    "\n",
    "    return TI_hh_2020\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "980f014c-ebd6-4b68-89bf-e96c1508448a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bellinzona</th>\n",
       "      <th>Blenio</th>\n",
       "      <th>Leventina</th>\n",
       "      <th>Locarno</th>\n",
       "      <th>Lugano</th>\n",
       "      <th>Mendrisio</th>\n",
       "      <th>Riviera</th>\n",
       "      <th>Vallemaggia</th>\n",
       "      <th>TI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hh</th>\n",
       "      <td>3.229900e+04</td>\n",
       "      <td>6.918000e+03</td>\n",
       "      <td>9.334000e+03</td>\n",
       "      <td>5.669800e+04</td>\n",
       "      <td>9.476600e+04</td>\n",
       "      <td>2.982800e+04</td>\n",
       "      <td>6.156000e+03</td>\n",
       "      <td>6.630000e+03</td>\n",
       "      <td>2.426290e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh_p</th>\n",
       "      <td>2.713116e+04</td>\n",
       "      <td>2.573496e+03</td>\n",
       "      <td>4.433650e+03</td>\n",
       "      <td>3.248795e+04</td>\n",
       "      <td>7.694999e+04</td>\n",
       "      <td>2.460810e+04</td>\n",
       "      <td>5.029452e+03</td>\n",
       "      <td>2.638740e+03</td>\n",
       "      <td>1.758525e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh_s</th>\n",
       "      <td>5.167840e+03</td>\n",
       "      <td>4.344504e+03</td>\n",
       "      <td>4.900350e+03</td>\n",
       "      <td>2.421005e+04</td>\n",
       "      <td>1.781601e+04</td>\n",
       "      <td>5.219900e+03</td>\n",
       "      <td>1.126548e+03</td>\n",
       "      <td>3.991260e+03</td>\n",
       "      <td>6.677646e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>res_elec</th>\n",
       "      <td>9.948357e+07</td>\n",
       "      <td>1.281684e+07</td>\n",
       "      <td>1.980609e+07</td>\n",
       "      <td>1.349949e+08</td>\n",
       "      <td>2.862707e+08</td>\n",
       "      <td>9.027070e+07</td>\n",
       "      <td>1.874764e+07</td>\n",
       "      <td>1.285095e+07</td>\n",
       "      <td>6.752414e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>res_sh</th>\n",
       "      <td>3.352842e+08</td>\n",
       "      <td>4.595342e+07</td>\n",
       "      <td>8.068091e+07</td>\n",
       "      <td>4.508242e+08</td>\n",
       "      <td>1.106362e+09</td>\n",
       "      <td>4.057988e+08</td>\n",
       "      <td>6.999013e+07</td>\n",
       "      <td>4.420108e+07</td>\n",
       "      <td>2.539095e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>res_dhw</th>\n",
       "      <td>5.699847e+07</td>\n",
       "      <td>6.446376e+06</td>\n",
       "      <td>1.110945e+07</td>\n",
       "      <td>7.553327e+07</td>\n",
       "      <td>1.831202e+08</td>\n",
       "      <td>6.029134e+07</td>\n",
       "      <td>1.092239e+07</td>\n",
       "      <td>6.454955e+06</td>\n",
       "      <td>4.108765e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Bellinzona        Blenio     Leventina       Locarno   \n",
       "hh        3.229900e+04  6.918000e+03  9.334000e+03  5.669800e+04  \\\n",
       "hh_p      2.713116e+04  2.573496e+03  4.433650e+03  3.248795e+04   \n",
       "hh_s      5.167840e+03  4.344504e+03  4.900350e+03  2.421005e+04   \n",
       "res_elec  9.948357e+07  1.281684e+07  1.980609e+07  1.349949e+08   \n",
       "res_sh    3.352842e+08  4.595342e+07  8.068091e+07  4.508242e+08   \n",
       "res_dhw   5.699847e+07  6.446376e+06  1.110945e+07  7.553327e+07   \n",
       "\n",
       "                Lugano     Mendrisio       Riviera   Vallemaggia            TI  \n",
       "hh        9.476600e+04  2.982800e+04  6.156000e+03  6.630000e+03  2.426290e+05  \n",
       "hh_p      7.694999e+04  2.460810e+04  5.029452e+03  2.638740e+03  1.758525e+05  \n",
       "hh_s      1.781601e+04  5.219900e+03  1.126548e+03  3.991260e+03  6.677646e+04  \n",
       "res_elec  2.862707e+08  9.027070e+07  1.874764e+07  1.285095e+07  6.752414e+08  \n",
       "res_sh    1.106362e+09  4.057988e+08  6.999013e+07  4.420108e+07  2.539095e+09  \n",
       "res_dhw   1.831202e+08  6.029134e+07  1.092239e+07  6.454955e+06  4.108765e+08  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RESIDENTIAL ELECTRICITY DEMAND PER DISTRICT - Getting the number of households according to ERA (EWID) - WORKS!\n",
    "## TEST combine with heating\n",
    "\n",
    "\n",
    "#TO-DO:\n",
    "# 1. DONE - combine with htg calc\n",
    "# 2. DONE - to_csv\n",
    "# 3. add a little bit of \"complexity\" - see if possible to seperate MFH and SFH and still get the right TI total res elect\n",
    "# 4. secondary homes - for further accuracy, not random sample ? \"Elles ont d’ailleurs un potentiel important, car les appartements en immeuble représentent plus de 75% du parc de résidence secondaire en Suisse https://makeheatsimple.ch/fr/2021/08/le-chauffage-des-residences-secondaires-a-portee-de-clic-meme-en-ppe/\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "sup_file = DATA_DIRECTORY / \"support_data\" \n",
    "# reading consumption data for SH and DHW per type of building and construction time\n",
    "data_sh = pd.read_csv(sup_file/\"demand_SH.csv\" , header=0, index_col=0)\n",
    "data_dhw = pd.read_csv(sup_file/\"demand_DHW.csv\", header=0, index_col=0)\n",
    "fehh = pd.read_csv(sup_file/\"fehh.csv\", header=0, index_col=0)\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "# Define constants\n",
    "EPSG_CODE = \"EPSG:2056\"\n",
    "# BUFFER_DISTANCE = 5\n",
    "GKLAS = [1110,1121,1122,1130, 1211, 1212, 1220, 1230, 1251, 1261, 1262, 1263, 1264, 1265, 1272, 1241, 1242, 1271, 1274, 1275, 1276, 1277, 1278, 1231, 1252, 1273]\n",
    "GKLAS_HTG = [1110, 1121, 1122, 1130, 1211, 1212, 1220, 1230, 1251, 1261, 1262, 1263, 1264, 1265, 1272, 1231]\n",
    "GKLAS_NOHTG = [1241, 1242, 1252, 1271, 1273, 1274, 1275, 1276, 1277, 1278]\n",
    "GBAUP = [8011,8012,8013,8014,8015,8016,8017,8018,8019,8020,8021,8022,8023]\n",
    "GSTAT = 1004 # existing\n",
    "\n",
    "NET_ERA_SHARE = 0.9 # SIA 416 - la superficie netta di un locale o un gruppo di locali corrisponde ca. al 90% della superficie del piano [SIA 380-4 (2006)]\n",
    "\n",
    "# SFH_ERA_FACTOR = 1.15 # ODIS\n",
    "# DFH_ERA_FACTOR = 1.2 # ODIS\n",
    "# MFH_ERA_FACTOR = 1.2 # ODIS\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "TI_res = pd.DataFrame()\n",
    "district_res = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "\n",
    "AVG_HH_ELEC_TI = 3499 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "avg_hh_MFH = get_hh_MFH()\n",
    "hh_stats = get_hh_stats() \n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]    \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    num = round(avg_hh_MFH.at[\"MFH_avg_households\",DISTRICT])\n",
    "\n",
    "    MFH_hh_list = generate_number_list(num)\n",
    "    MFH_avg = str(MFH_hh_list)\n",
    "\n",
    "#    print(DISTRICT, MFH_avg)\n",
    "\n",
    "    for com_num in communes:\n",
    "        fileloc =  CANTON_CSV_DIRECTORY\n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(fileloc/filename_data).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        # Keep only existing residential buildings\n",
    "        data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "        data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "        data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "        \n",
    "        # Setting up the base year to 2020, so drop buildings that where buit after 2020 \n",
    "        data_res = data_res.drop(data_res[(data_res[\"gbauj\"]==2021)|(data_res[\"gbauj\"]==2022)|(data_res[\"gbauj\"]==2022)].index)\n",
    "\n",
    "        data_res[\"garea_fill\"] = data_res[\"garea\"]\n",
    "        data_res[\"gbaup_fill\"] = data_res[\"gbaup\"]\n",
    "        data_res[\"gastw_fill\"] = data_res[\"gastw\"]\n",
    "        data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "        \n",
    "        data_res = data_res.reset_index()\n",
    "\n",
    "        age_group = data_res[[\"gkat\", \"gklas\", \"gbaup\"]]\n",
    "        age_pivot = pd.pivot_table(age_group, values='gbaup', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        age_pivot = age_pivot.round(0)\n",
    "        age_col = list(age_pivot.columns)\n",
    "        for code in GKLAS:\n",
    "            if code in age_col:\n",
    "                pass\n",
    "            else:                \n",
    "                age_pivot[code] = 8016.0\n",
    "\n",
    "        size_group = data_res[[\"gkat\", \"gklas\", \"garea\"]]\n",
    "        size_pivot = pd.pivot_table(size_group, values='garea', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        size_pivot = size_pivot.round(0)\n",
    "        size_col = list(size_pivot.columns)\n",
    "        \n",
    "        for g in range(len(data_res)):\n",
    "            if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "            elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                data_res.loc[g,\"gklas_fill\"] = 1122 # 63% of known 1040\n",
    "\n",
    "            data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "\n",
    "                    \n",
    "            if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "                if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1', '2']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "                    data_res.at[g,\"ewid\"] = MFH_avg # GET real median\n",
    "            else:\n",
    "                pass\n",
    "      \n",
    "            if np.isnan(data_res.at[g,\"gbaup\"]):             \n",
    "                data_res.at[g,\"gbaup_fill\"] = age_pivot.loc[data_res.at[g,\"gkat\"],data_res.at[g,\"gklas_fill\"]]\n",
    "\n",
    "            if np.isnan(data_res.at[g,\"garea\"]):\n",
    "                if data_res.at[g,\"gklas_fill\"] in GKLAS_NOHTG:\n",
    "                    data_res.at[g,\"garea_fill\"] = 0\n",
    "                else:    \n",
    "                    if data_res.at[g,\"gklas_fill\"] in size_col:\n",
    "                        pass\n",
    "                    else:   \n",
    "                        if data_res.at[g,\"gklas_fill\"] == 1130.0:\n",
    "                            size_pivot.loc[1040.0,1130.0] = size_pivot.loc[1040.0, 1211.0]\n",
    "                        else:\n",
    "                            pass\n",
    "                    data_res.at[g,\"garea_fill\"] = size_pivot.loc[(data_res.at[g,\"gkat\"],data_res.at[g,\"gklas_fill\"])]\n",
    "\n",
    "\n",
    "        data_res[\"garea_fill\"] = data_res[\"garea_fill\"].astype(np.int64)\n",
    "        data_res[\"gklas_fill\"] = data_res[\"gklas_fill\"].astype(np.int64)\n",
    "        data_res[\"gbaup_fill\"] = data_res[\"gbaup_fill\"].astype(np.int64)\n",
    "        data_res[\"gastw_fill\"] = data_res[\"gastw_fill\"].fillna(value = 1)\n",
    "        \n",
    "        for g in range(len(data_res)):\n",
    "            ewid_count.at[g,\"ewid\"] = ast.literal_eval(data_res.at[g,\"ewid\"])      \n",
    "\n",
    "        data_res[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "        \n",
    "        # sum the ewid_len per egid\n",
    "        grouped = data_res.groupby('egid')[\"ewid_len\"].sum().reset_index()\n",
    "        grouped = pd.DataFrame(grouped)\n",
    "        \n",
    "        #drop duplicate egids\n",
    "        data_res = data_res.drop_duplicates(subset=['egid'])\n",
    "        \n",
    "        # Merge dataframes based on column 'egid'\n",
    "        data_res = data_res.merge(grouped, on='egid', how='left', suffixes=('_original', '_replacement'))\n",
    "\n",
    "        # Update 'ewid' values based on the replacement values\n",
    "        data_res['ewid_len'] = data_res['ewid_len_replacement'].fillna(data_res['ewid_len_original'])\n",
    "\n",
    "        # Drop the original and replacement columns\n",
    "        data_res = data_res.drop(columns=['ewid_len_original', 'ewid_len_replacement'])\n",
    "                \n",
    "        for g in range(len(data_res)):\n",
    "            data_res.at[g,'k_SH'] = data_sh.loc[(data_res.at[g,\"gbaup_fill\"],str(data_res.at[g,\"gklas_fill\"]))]\n",
    "            data_res.at[g,'k_DHW'] = data_dhw.loc[(data_res.at[g,\"gbaup_fill\"],str(data_res.at[g,\"gklas_fill\"]))]\n",
    "            data_res.at[g,'k_fehh'] = fehh.loc[(data_res.at[g,\"gbaup_fill\"],str(data_res.at[g,\"gklas_fill\"]))]\n",
    "\n",
    "        district_res = pd.concat([district_res,data_res],ignore_index=True)\n",
    "    \n",
    "    sample_size = int(len(district_res)*(1-UNOCC))\n",
    "\n",
    "    # Take a random sample of 90% of the rows\n",
    "    district_res_occ = district_res.sample(n=sample_size, random_state=1) #random state can be changed, otherwise always same sample\n",
    "    district_res_occ.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    d_hh = (district_res_occ[\"ewid_len\"].sum())\n",
    "    \n",
    "    #d_hh = d_hh*hh_stats.at[\"hh2023_diff\",DISTRICT] #\"hh2023_diff\" = ratio of residences in 2020/2023\n",
    "    d_hh_p = d_hh*(1-hh_stats.at[\"hh2020_p_ratio\",DISTRICT]) # (1-\"hh2020_p_ratio\"= primary residences \n",
    "    d_hh_s = d_hh*(hh_stats.at[\"hh2020_p_ratio\",DISTRICT])  # \"hh2020_p_ratio\" = ratio of secondary residences \n",
    "    \n",
    "    num_rows = len(district_res_occ)\n",
    "    num_ones = round(len(district_res_occ)*(1-hh_stats.at[\"hh2020_p_ratio\",DISTRICT]))\n",
    "\n",
    "    rand_values = np.array([1] * num_ones + [1/4] * (num_rows - num_ones))\n",
    "    np.random.shuffle(rand_values)\n",
    "\n",
    "    # Assign the generated random values to the \"primary\" column\n",
    "    district_res_occ['primary'] = rand_values\n",
    "\n",
    "    # calculating building attributes and demand\n",
    "    district_res_occ['ERA'] = district_res_occ['garea_fill']*district_res_occ['gastw_fill']*NET_ERA_SHARE\n",
    "    \n",
    "    for g in range(len(district_res_occ)):\n",
    "        if district_res_occ.at[g,'primary'] == 1:\n",
    "            district_res_occ.at[g,'SH'] = district_res_occ.at[g,'k_SH']*district_res_occ.at[g,'ERA']*district_res_occ.at[g,'primary']\n",
    "        else:\n",
    "            district_res_occ.at[g,'SH'] = district_res_occ.at[g,'k_SH']*district_res_occ.at[g,'ERA']*(district_res_occ.at[g,'primary']/3)\n",
    "            \n",
    "    district_res_occ['DHW'] = district_res_occ['k_DHW']*district_res_occ['ERA']*district_res_occ['primary']\n",
    "    district_res_occ['ELEC'] = district_res_occ['ewid_len']*AVG_HH_ELEC_TI*district_res_occ['primary']\n",
    "\n",
    "    \n",
    "    d_elec = district_res_occ[\"ELEC\"].sum() \n",
    "    d_sh = district_res_occ[\"SH\"].sum() \n",
    "    d_dhw = district_res_occ[\"DHW\"].sum() \n",
    "    \n",
    "    d = pd.DataFrame([d_hh, d_hh_p, d_hh_s, d_elec, d_sh, d_dhw], index=[\"hh\",\"hh_p\",\"hh_s\",\"res_elec\",\"res_sh\",\"res_dhw\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_res = pd.concat([TI_res,d], axis=1)\n",
    "    district_res = district_res[0:0]\n",
    "\n",
    "TI_res[\"TI\"] = TI_res.sum(axis=1)\n",
    "\n",
    "path = str(DATA_DIRECTORY)+ \"/results/residential_demand.csv\"\n",
    "filepath = pathlib.Path(path)  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "TI_res.to_csv(filepath, sep=\";\", encoding='utf-8-sig', index_label='index')\n",
    "TI_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef8109-b46f-49a1-9abe-d8e0208fee51",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b684c9-cfe1-4bf0-b549-60612f4dd869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bellinzona</th>\n",
       "      <th>Blenio</th>\n",
       "      <th>Leventina</th>\n",
       "      <th>Locarno</th>\n",
       "      <th>Lugano</th>\n",
       "      <th>Mendrisio</th>\n",
       "      <th>Riviera</th>\n",
       "      <th>Vallemaggia</th>\n",
       "      <th>TI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hh</th>\n",
       "      <td>3.164941e+04</td>\n",
       "      <td>6.739898e+03</td>\n",
       "      <td>9.178087e+03</td>\n",
       "      <td>5.413970e+04</td>\n",
       "      <td>8.819780e+04</td>\n",
       "      <td>2.955804e+04</td>\n",
       "      <td>5.984407e+03</td>\n",
       "      <td>6.408237e+03</td>\n",
       "      <td>2.318556e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh_p</th>\n",
       "      <td>2.658550e+04</td>\n",
       "      <td>2.507242e+03</td>\n",
       "      <td>4.359592e+03</td>\n",
       "      <td>3.102205e+04</td>\n",
       "      <td>7.161662e+04</td>\n",
       "      <td>2.438539e+04</td>\n",
       "      <td>4.889260e+03</td>\n",
       "      <td>2.550478e+03</td>\n",
       "      <td>1.679161e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh_s</th>\n",
       "      <td>5.063905e+03</td>\n",
       "      <td>4.232656e+03</td>\n",
       "      <td>4.818496e+03</td>\n",
       "      <td>2.311765e+04</td>\n",
       "      <td>1.658119e+04</td>\n",
       "      <td>5.172658e+03</td>\n",
       "      <td>1.095146e+03</td>\n",
       "      <td>3.857759e+03</td>\n",
       "      <td>6.393946e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>res_elec</th>\n",
       "      <td>1.020478e+08</td>\n",
       "      <td>1.306365e+07</td>\n",
       "      <td>2.038729e+07</td>\n",
       "      <td>1.348405e+08</td>\n",
       "      <td>2.775916e+08</td>\n",
       "      <td>9.408621e+07</td>\n",
       "      <td>1.891740e+07</td>\n",
       "      <td>1.287866e+07</td>\n",
       "      <td>6.738132e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Bellinzona        Blenio     Leventina       Locarno   \n",
       "hh        3.164941e+04  6.739898e+03  9.178087e+03  5.413970e+04  \\\n",
       "hh_p      2.658550e+04  2.507242e+03  4.359592e+03  3.102205e+04   \n",
       "hh_s      5.063905e+03  4.232656e+03  4.818496e+03  2.311765e+04   \n",
       "res_elec  1.020478e+08  1.306365e+07  2.038729e+07  1.348405e+08   \n",
       "\n",
       "                Lugano     Mendrisio       Riviera   Vallemaggia            TI  \n",
       "hh        8.819780e+04  2.955804e+04  5.984407e+03  6.408237e+03  2.318556e+05  \n",
       "hh_p      7.161662e+04  2.438539e+04  4.889260e+03  2.550478e+03  1.679161e+05  \n",
       "hh_s      1.658119e+04  5.172658e+03  1.095146e+03  3.857759e+03  6.393946e+04  \n",
       "res_elec  2.775916e+08  9.408621e+07  1.891740e+07  1.287866e+07  6.738132e+08  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ELECTRICITY DEMAND PER DISTRICT - Getting the number of households according to ERA (EWID) - WORKS\n",
    "# NOTE: there are egid duplicates but each have different ewids, for the purpose of elec keep all, but need to write code that combines the ewids for each egid. For heating work with EGID since the garea is duplicated\n",
    "\n",
    "#TO-DO:\n",
    "# 1. combine with htg calc\n",
    "# 2. to_csv\n",
    "# 3. add a little bit of \"complexity\" - see if possible to seperate MFH and SFH and still get the right TI total res elect\n",
    "# 4. secondary homes - reduce consumption\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "TI_elec = pd.DataFrame()\n",
    "district_res_elec = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "\n",
    "AVG_HH_ELEC_TI = 3664 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "avg_hh_MFH = get_hh_MFH()\n",
    "hh_stats = get_hh_stats() \n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]    \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    num = round(avg_hh_MFH.at[\"MFH_avg_households\",DISTRICT])\n",
    "\n",
    "    MFH_hh_list = generate_number_list(num)\n",
    "    MFH_avg = str(MFH_hh_list)\n",
    "\n",
    "#    print(DISTRICT, MFH_avg)\n",
    "\n",
    "    for com_num in communes:\n",
    "        fileloc =  CANTON_CSV_DIRECTORY\n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(fileloc/filename_data).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "        data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "        data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "\n",
    "        data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "        data_res = data_res.reset_index()\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "            if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "            elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                data_res.loc[g,\"gklas_fill\"] = 1122 # 63% of known 1040\n",
    "\n",
    "        data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "            if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "                if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1', '2']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "                    data_res.at[g,\"ewid\"] = MFH_avg # GET real median\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "            ewid_count.at[g,\"ewid\"] = ast.literal_eval(data_res.at[g,\"ewid\"])      \n",
    "\n",
    "        data_res[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "        district_res_elec = pd.concat([district_res_elec,data_res])\n",
    "\n",
    "    d_hh = (1-UNOCC)*(district_res_elec[\"ewid_len\"].sum()) #\n",
    "    d_hh = d_hh*hh_stats.at[\"hh2023_diff\",DISTRICT]\n",
    "    d_hh_p = d_hh*(1-hh_stats.at[\"hh2020_p_ratio\",DISTRICT])\n",
    "    d_hh_s = d_hh*(hh_stats.at[\"hh2020_p_ratio\",DISTRICT])\n",
    "    d_elec = d_hh_p*AVG_HH_ELEC_TI + d_hh_s*(AVG_HH_ELEC_TI/4)\n",
    "    d = pd.DataFrame([d_hh, d_hh_p, d_hh_s, d_elec], index=[\"hh\",\"hh_p\",\"hh_s\",\"res_elec\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_elec = pd.concat([TI_elec,d], axis=1)\n",
    "    district_res_elec = district_res_elec[0:0]\n",
    "\n",
    "TI_elec[\"TI\"] = TI_elec.sum(axis=1)\n",
    "TI_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c858d0-0dec-43e8-b36d-2a8475a3d05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = int(len(data_res) * 0.9)\n",
    "\n",
    "# Take a random sample of 90% of the rows\n",
    "sample_df = data_res.sample(n=sample_size, random_state=42)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088abfd-2752-4ddb-806c-f4c78bc80a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of households per district using https://data.geo.admin.ch/browser/index.html#/collections/ch.are.wohnungsinventar-zweitwohnungsanteil?.language=en\n",
    "\n",
    "# 1. GET Secondary households per commune\n",
    "# 2. GET difference in number of households 2020 and 2023\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "\n",
    "#SELECT CANTON\n",
    "#CANTON = \"Genève\"\n",
    "CANTON = \"Ticino\"\n",
    "\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6805f3c-75e1-47de-abc5-42e81f81f5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TI_hh_2020 = pd.DataFrame()\n",
    "district_hh_2020 = pd.DataFrame()\n",
    "district_hh_2023 = pd.DataFrame()\n",
    "\n",
    "dis_num = LUGANO_UST\n",
    "communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"]\n",
    "\n",
    "for com_name in communes:\n",
    "    #print(\"comune: \", com_num)\n",
    "    hh_2020 = data_households_2020.loc[data_households_2020[\"Name\"] == com_name]#\n",
    "    hh_2020 = hh_2020.reset_index()\n",
    "    \n",
    "    hh_2023 = data_households_2023.loc[data_households_2023[\"Name\"] == com_name]#\n",
    "    hh_2023 = hh_2023.reset_index()\n",
    "\n",
    "    district_hh_2020 = pd.concat([district_hh_2020, hh_2020])\n",
    "    district_hh_2023 = pd.concat([district_hh_2023, hh_2023])\n",
    "\n",
    "dis_tot_hh_2020 = district_hh_2020[\"ZWG_3150\"].sum()\n",
    "dis_tot_primary_2020 = district_hh_2020[\"ZWG_3010\"].sum() + district_hh_2020[\"ZWG_3100\"].sum()\n",
    "calc_dist_secondary_2020 = round(((dis_tot_hh_2020 - dis_tot_primary_2020)/dis_tot_hh_2020),3)\n",
    "ratio_secondary_mean_2020 = district_hh_2020[\"ZWG_3120\"].mean()\n",
    "ratio_secondary_median_2020 = district_hh_2020[\"ZWG_3120\"].median()\n",
    "\n",
    "dis_tot_hh_2023 = district_hh_2023[\"ZWG_3150\"].sum()\n",
    "dis_tot_primary_2023 = district_hh_2023[\"ZWG_3010\"].sum() + district_hh_2023[\"ZWG_3100\"].sum()\n",
    "\n",
    "hh_diff = round((dis_tot_hh_2023 - dis_tot_hh_2020),3)\n",
    "\n",
    "d_hh = pd.DataFrame([calc_dist_secondary_2020, hh_diff], index=[\"hh2020_p_ratio\", \"hh2023_diff\"], columns=[DISTRICT])\n",
    "\n",
    "TI_hh_2020 = pd.concat([TI_hh_2020,d_hh], axis=1)\n",
    "district_hh = district_hh[0:0]\n",
    "\n",
    "#print(\"calc = \", calc_dist_secondary_2020, \"mean = \", ratio_secondary_mean_2020, \"median = \", ratio_secondary_median_2020)\n",
    "#print(\"tot_hh_2020 = \",dis_tot_hh_2020 ,\"primary_hh_2020 = \",dis_tot_primary_2020, \"ratio = \", dis_tot_primary_2020/dis_tot_hh_2020 )\n",
    "#print(\"tot_hh_2023 = \",dis_tot_hh_2023 ,\"primary_hh_2023 = \",dis_tot_primary_2023, \"ratio = \", dis_tot_primary_2023/dis_tot_hh_2023)\n",
    "\n",
    "TI_hh_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4abf2529-f271-4655-ac42-e5df0e06688a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bellinzona</th>\n",
       "      <th>Blenio</th>\n",
       "      <th>Leventina</th>\n",
       "      <th>Locarno</th>\n",
       "      <th>Lugano</th>\n",
       "      <th>Mendrisio</th>\n",
       "      <th>Riviera</th>\n",
       "      <th>Vallemaggia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hh2020_tot</th>\n",
       "      <td>31672.000</td>\n",
       "      <td>6981.000</td>\n",
       "      <td>9186.000</td>\n",
       "      <td>55812.000</td>\n",
       "      <td>89054.000</td>\n",
       "      <td>29599.000</td>\n",
       "      <td>5779.000</td>\n",
       "      <td>6659.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh2020_p</th>\n",
       "      <td>26617.000</td>\n",
       "      <td>2595.000</td>\n",
       "      <td>4363.000</td>\n",
       "      <td>31979.000</td>\n",
       "      <td>72323.000</td>\n",
       "      <td>24425.000</td>\n",
       "      <td>4720.000</td>\n",
       "      <td>2650.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh2023_tot</th>\n",
       "      <td>32968.000</td>\n",
       "      <td>7180.000</td>\n",
       "      <td>9385.000</td>\n",
       "      <td>59265.000</td>\n",
       "      <td>97105.000</td>\n",
       "      <td>30435.000</td>\n",
       "      <td>6027.000</td>\n",
       "      <td>6950.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh2020_p_ratio</th>\n",
       "      <td>0.160</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hh2023_diff</th>\n",
       "      <td>0.961</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Bellinzona    Blenio  Leventina    Locarno     Lugano   \n",
       "hh2020_tot       31672.000  6981.000   9186.000  55812.000  89054.000  \\\n",
       "hh2020_p         26617.000  2595.000   4363.000  31979.000  72323.000   \n",
       "hh2023_tot       32968.000  7180.000   9385.000  59265.000  97105.000   \n",
       "hh2020_p_ratio       0.160     0.628      0.525      0.427      0.188   \n",
       "hh2023_diff          0.961     0.972      0.979      0.942      0.917   \n",
       "\n",
       "                Mendrisio   Riviera  Vallemaggia  \n",
       "hh2020_tot      29599.000  5779.000     6659.000  \n",
       "hh2020_p        24425.000  4720.000     2650.000  \n",
       "hh2023_tot      30435.000  6027.000     6950.000  \n",
       "hh2020_p_ratio      0.175     0.183        0.602  \n",
       "hh2023_diff         0.973     0.959        0.958  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GET ratios for primary households and difference between 2023 and 2020 - WORKS!\n",
    "sup_file = DATA_DIRECTORY / \"support_data\" \n",
    "\n",
    "data_households_2020 = pd.read_excel(sup_file/\"household_inventory_2020-10_2056.xlsx\", sheet_name=\"ZWG_2020_Q3\" , header=0, index_col=0)\n",
    "data_households_2023 = pd.read_excel(sup_file/\"household_inventory_2023-03_2056.xlsx\", sheet_name=\"ZWG_2023\" , header=0, index_col=0)\n",
    "data_households_2023.head()\n",
    "\n",
    "TI_hh_2020 = pd.DataFrame()\n",
    "district_hh_2020 = pd.DataFrame()\n",
    "district_hh_2023 = pd.DataFrame()\n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"]    \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "\n",
    "    for com_name in communes:\n",
    "        hh_2020 = data_households_2020.loc[data_households_2020[\"Name\"] == com_name]#\n",
    "        hh_2020 = hh_2020.reset_index()\n",
    "\n",
    "        hh_2023 = data_households_2023.loc[data_households_2023[\"Name\"] == com_name]#\n",
    "        hh_2023 = hh_2023.reset_index()\n",
    "\n",
    "        district_hh_2020 = pd.concat([district_hh_2020, hh_2020])\n",
    "        district_hh_2023 = pd.concat([district_hh_2023, hh_2023])\n",
    "\n",
    "    dis_tot_hh_2020 = district_hh_2020[\"ZWG_3150\"].sum()\n",
    "    dis_tot_primary_2020 = district_hh_2020[\"ZWG_3010\"].sum() + district_hh_2020[\"ZWG_3100\"].sum()\n",
    "    \n",
    "    calc_dist_secondary_2020 = round(((dis_tot_hh_2020 - dis_tot_primary_2020)/dis_tot_hh_2020),3)\n",
    "    \n",
    "    ratio_secondary_mean_2020 = district_hh_2020[\"ZWG_3120\"].mean()\n",
    "    ratio_secondary_median_2020 = district_hh_2020[\"ZWG_3120\"].median()\n",
    "\n",
    "    dis_tot_hh_2023 = district_hh_2023[\"ZWG_3150\"].sum()\n",
    "    dis_tot_primary_2023 = district_hh_2023[\"ZWG_3010\"].sum() + district_hh_2023[\"ZWG_3100\"].sum()\n",
    "\n",
    "    hh_diff = round((dis_tot_hh_2020/dis_tot_hh_2023),3)\n",
    "\n",
    "    d_hh = pd.DataFrame([dis_tot_hh_2020, dis_tot_primary_2020, dis_tot_hh_2023, calc_dist_secondary_2020, hh_diff], index=[\"hh2020_tot\", \"hh2020_p\",\"hh2023_tot\",\"hh2020_p_ratio\", \"hh2023_diff\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_hh_2020 = pd.concat([TI_hh_2020,d_hh], axis=1)\n",
    "    district_hh_2020 = district_hh_2020[0:0]\n",
    "    district_hh_2023 = district_hh_2023[0:0]\n",
    "\n",
    "path = str(DATA_DIRECTORY)+ \"/support_data/\"+ \"housing_inventory_stats.csv\"\n",
    "filepath = pathlib.Path(path)  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "TI_hh_2020.to_csv(filepath, sep=\";\", encoding='utf-8-sig', index_label='index')\n",
    "TI_hh_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c4724-0330-417d-80ce-99469cbafee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of households per district - WORKS\n",
    "import ast\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "TI_hh = pd.DataFrame()\n",
    "district_hh = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "\n",
    "\n",
    "AVG_HH_ELEC_TI = 2741 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "#dis_num = BELLINZONA_UST\n",
    "#com_num = 5001\n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]\n",
    "\n",
    "    for com_num in communes:\n",
    "        #print(\"comune: \", com_num)\n",
    "\n",
    "        fileloc =  CANTON_CSV_DIRECTORY\n",
    "\n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(fileloc/filename_data).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "        data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "        data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "        data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "        data_res = data_res.reset_index(drop=True)\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "             if (np.isnan(data_res.at[g,\"gklas\"])):\n",
    "                if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                    data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "                elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                    data_res.loc[g,\"gklas_fill\"] = 1122\n",
    "\n",
    "        data_res[\"egid_duplicate\"] = data_res.duplicated(subset = [\"egid\"], keep = False)\n",
    "\n",
    "        no_nan = data_res.dropna(subset = [\"ewid\"])\n",
    "        no_nan = no_nan.reset_index(drop=True) #,, inplace=True\n",
    "\n",
    "        for g in range(len(no_nan)):\n",
    "            ewid_count.at[g,\"ewid\"] = ast.literal_eval(no_nan.at[g,\"ewid\"]) \n",
    "\n",
    "        no_nan[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "        duplicates = no_nan.loc[no_nan[\"egid_duplicate\"] == True]\n",
    "        group = duplicates.groupby(\"egid\")[\"ewid_len\"].sum()\n",
    "        group = pd.DataFrame(group)\n",
    "\n",
    "        # Replace ewid_len values in DataFrame A based on DataFrame B\n",
    "        no_nan['ewid_len'] = no_nan.apply(lambda row: group.loc[row['egid'], 'ewid_len'] if row['egid'] in group.index else row['ewid_len'], axis=1)\n",
    "        no_nan.drop_duplicates(subset =[\"egid\"], keep=\"first\", inplace = True)\n",
    "\n",
    "        district_hh = pd.concat([district_hh,no_nan])\n",
    "    \n",
    "    abi_num = district_hh [[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "    abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "    abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "    abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "    med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "    med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "    MFH = med.loc[1122.0]\n",
    "    MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "\n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    hh = pd.DataFrame([MFH_avghh], index=[\"MFH_avg_households\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_hh = pd.concat([TI_hh,hh], axis=1)\n",
    "    district_hh = district_hh[0:0]\n",
    "\n",
    "TI_hh[\"TI\"] = TI_hh.mean(axis=1)\n",
    "TI_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d77763-476c-42e4-a9e7-fa730e0e1c84",
   "metadata": {},
   "source": [
    "The municipalities have to show the total number of apartments and first apartments in the apartment inventory, however, there is no obligation to declare first apartments, equivalent apartments or second apartments as such in the RBD. Therefore, the data from the apartment inventory regarding second apartments cannot be compared with data from other municipalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf587-5455-4da7-83cb-933ac2d115de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ELECTRICITY DEMAND PER DISTRICT - Getting the number of households according to ERA (EWID) - WORKS\n",
    "# 1. there are egid duplicates but each have different ewids, for the purpose of elec keep all, but need to write code that combines the ewids for each egid. For heating work with EGID since the garea is duplicated\n",
    "# 2. combine with htg calc\n",
    "# 3. to_csv\n",
    "# 4. add a little bit of \"complexity\" - see if possible to seperate MFH and SFH and still get the right TI total res elect\n",
    "\n",
    "import ast\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "TI_elec = pd.DataFrame()\n",
    "district_res_elec = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "#dis_num = 2101\n",
    "\n",
    "AVG_HH_ELEC_TI = 2741 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]\n",
    "\n",
    "    for com_num in communes:\n",
    "        #print(\"comune: \", com_num)\n",
    "        fileloc =  CANTON_CSV_DIRECTORY\n",
    "\n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "        \n",
    "        if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(fileloc/filename_data).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        \n",
    "        data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "        data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "        data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "\n",
    "        data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "        data_res = data_res.reset_index()\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "            if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "            elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                data_res.loc[g,\"gklas_fill\"] = 1122 # 63% of known 1040\n",
    "\n",
    "        data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "            if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "                if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1', '2']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1', '2', '3', '4', '5', '6', '7']\" # GET real median\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "            ewid_count.at[g,\"ewid\"] = ast.literal_eval(data_res.at[g,\"ewid\"])      \n",
    "\n",
    "        data_res[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "        district_res_elec = pd.concat([district_res_elec,data_res])\n",
    "        \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    d_hh = (1-UNOCC)*(district_res_elec[\"ewid_len\"].sum()) #\n",
    "    d_elec = d_hh*AVG_HH_ELEC_TI\n",
    "    d = pd.DataFrame([d_hh, d_elec], index=[\"hh\",\"res_elec\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_elec = pd.concat([TI_elec,d], axis=1)\n",
    "    district_res_elec = district_res_elec[0:0]\n",
    "\n",
    "TI_elec[\"TI\"] = TI_elec.sum(axis=1)\n",
    "TI_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd0cd8-621b-44fa-841e-fc413253d81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TI_hh = pd.DataFrame()\n",
    "district_hh = pd.DataFrame()\n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"]\n",
    "\n",
    "    for com_name in communes:\n",
    "        #print(\"comune: \", com_num)\n",
    "        hh = data_households.loc[data_households[\"Name\"] == com_name]#\n",
    "        hh = hh.reset_index(drop=True)\n",
    "        \n",
    "        district_hh = pd.concat([district_hh, hh])\n",
    "        \n",
    "    DISTRICT_NAME = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "\n",
    "    TI_hh = pd.concat([TI_hh,hh], axis=1)\n",
    "    district_hh = district_hh[0:0]\n",
    "\n",
    "TI_hh[\"TI\"] = TI_hh.mean(axis=1)\n",
    "TI_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155f5b7-f1f3-4d68-9ded-62f9c0ae87ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of households per district using rea data\n",
    "import ast\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "TI_hh = pd.DataFrame()\n",
    "district_hh = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "\n",
    "\n",
    "AVG_HH_ELEC_TI = 2741 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "#dis_num = BELLINZONA_UST\n",
    "#com_num = 5001\n",
    "\n",
    "for dis_num in UST_district:\n",
    "    communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]\n",
    "\n",
    "    for com_num in communes:\n",
    "        #print(\"comune: \", com_num)\n",
    "\n",
    "        fileloc =  CANTON_CSV_DIRECTORY\n",
    "\n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(fileloc/filename_data).exists():\n",
    "            data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "        data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "        data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "        data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "        data_res = data_res.reset_index(drop=True)\n",
    "\n",
    "        for g in range(len(data_res)):\n",
    "             if (np.isnan(data_res.at[g,\"gklas\"])):\n",
    "                if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                    data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "                elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                    data_res.loc[g,\"gklas_fill\"] = 1122\n",
    "\n",
    "        data_res[\"egid_duplicate\"] = data_res.duplicated(subset = [\"egid\"], keep = False)\n",
    "\n",
    "        no_nan = data_res.dropna(subset = [\"ewid\"])\n",
    "        no_nan = no_nan.reset_index(drop=True) #,, inplace=True\n",
    "\n",
    "        for g in range(len(no_nan)):\n",
    "            ewid_count.at[g,\"ewid\"] = ast.literal_eval(no_nan.at[g,\"ewid\"]) \n",
    "\n",
    "        no_nan[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "        duplicates = no_nan.loc[no_nan[\"egid_duplicate\"] == True]\n",
    "        group = duplicates.groupby(\"egid\")[\"ewid_len\"].sum()\n",
    "        group = pd.DataFrame(group)\n",
    "\n",
    "        # Replace ewid_len values in DataFrame A based on DataFrame B\n",
    "        no_nan['ewid_len'] = no_nan.apply(lambda row: group.loc[row['egid'], 'ewid_len'] if row['egid'] in group.index else row['ewid_len'], axis=1)\n",
    "        no_nan.drop_duplicates(subset =[\"egid\"], keep=\"first\", inplace = True)\n",
    "        \n",
    "        abi_num = no_nan[[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "        abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "        abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "        abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "        med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "        med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "        MFH = med.loc[1122.0]\n",
    "        MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "\n",
    "        data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "        \n",
    "        for g in range(len(data_res)):\n",
    "            if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "                if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "                    data_res.at[g,\"ewid_len\"] = 1\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "                    data_res.at[g,\"ewid_len\"] = 2\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "                    data_res.at[g,\"ewid_len\"] = MFH_avghh # GET real median\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        district_hh = pd.concat([district_hh,data_res])\n",
    "    \n",
    "    abi_num = district_hh[[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "    abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "    abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "    abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "    med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "    med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "    print(med)\n",
    "    MFH = med.loc[1122]\n",
    "    MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "\n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    hh = pd.DataFrame([MFH_avghh], index=[\"MFH_avg_households\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_hh = pd.concat([TI_hh,hh], axis=1)\n",
    "    district_hh = district_hh[0:0]\n",
    "\n",
    "TI_hh[\"TI\"] = TI_hh.mean(axis=1)\n",
    "TI_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fd87b-dde5-4cc3-8461-25042e13c33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of households per egid - per comune - WORKS\n",
    "import ast\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "TI_elec = pd.DataFrame()\n",
    "district_res_elec = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "\n",
    "\n",
    "AVG_HH_ELEC_TI = 2741 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "dis_num = BELLINZONA_UST\n",
    "com_num = 5001\n",
    "#for dis_num in UST_district:\n",
    "# communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]\n",
    "\n",
    "# for com_num in communes:\n",
    "    #print(\"comune: \", com_num)\n",
    "    \n",
    "fileloc =  CANTON_CSV_DIRECTORY\n",
    "\n",
    "filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "    data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "    #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "elif pathlib.Path(fileloc/filename_data).exists():\n",
    "    data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "    #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "data_res = data_res.reset_index(drop=True)\n",
    "\n",
    "for g in range(len(data_res)):\n",
    "     if (np.isnan(data_res.at[g,\"gklas\"])):\n",
    "        if (data_res.at[g,\"gkat\"] == 1020):\n",
    "            data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "        elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "            data_res.loc[g,\"gklas_fill\"] = 1122\n",
    "\n",
    "data_res[\"egid_duplicate\"] = data_res.duplicated(subset = [\"egid\"], keep = False)\n",
    "\n",
    "no_nan = data_res.dropna(subset = [\"ewid\"])\n",
    "no_nan = no_nan.reset_index(drop=True) #,, inplace=True\n",
    "\n",
    "for g in range(len(no_nan)):\n",
    "    ewid_count.at[g,\"ewid\"] = ast.literal_eval(no_nan.at[g,\"ewid\"]) \n",
    "\n",
    "no_nan[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "duplicates = no_nan.loc[no_nan[\"egid_duplicate\"] == True]\n",
    "group = duplicates.groupby(\"egid\")[\"ewid_len\"].sum()\n",
    "group = pd.DataFrame(group)\n",
    "\n",
    "# Replace ewid_len values in DataFrame A based on DataFrame B\n",
    "no_nan['ewid_len'] = no_nan.apply(lambda row: group.loc[row['egid'], 'ewid_len'] if row['egid'] in group.index else row['ewid_len'], axis=1)\n",
    "no_nan.drop_duplicates(subset =[\"egid\"], keep=\"first\", inplace = True)\n",
    "\n",
    "abi_num = no_nan[[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "MFH = med.loc[1122.0]\n",
    "#MFH\n",
    "MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "#MFH_avghh\n",
    "data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "        \n",
    "for g in range(len(data_res)):\n",
    "    if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "        if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "            data_res.at[g,\"ewid_len\"] = 1\n",
    "        elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "            data_res.at[g,\"ewid_len\"] = 2\n",
    "        elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "            data_res.at[g,\"ewid_len\"] = MFH_avghh # GET real median\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "hh_num = data_res[[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "hh_num = hh_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "hh_num = hh_num.rename(columns={\"egid\":\"count\"})\n",
    "hh_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "hh_med = pd.DataFrame(hh_num).reset_index(level=\"number of households\")\n",
    "hh_med[\"TOT\"] = hh_med[\"number of households\"]*med[\"count\"]\n",
    "hh_MFH = hh_med.loc[1122.0]\n",
    "#MFH\n",
    "hh_MFH_avghh = hh_MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "#district_hh = pd.concat([district_hh,data_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891f716-78e7-42e0-b710-263b6706590f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2196c5-764b-48e3-8a77-05ac59689e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abi_num = no_nan[[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "abi_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3c394-cc45-4318-8007-2844c0bcfd21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "MFH = med.loc[1122.0]\n",
    "MFH\n",
    "MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "MFH_avghh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58b31f-0826-48ba-a43d-3b0f7846624d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileDir = DATA_DIRECTORY / \"support_data\" \n",
    "data_s123 = pd.read_csv(fileDir/\"S1_S2_S3.csv\" , header=0, index_col=0)\n",
    "data_s123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8343b-b73b-4830-a0ef-2fe98941bea5",
   "metadata": {},
   "source": [
    "### S1 -AGRICULTURE, S2 -INDUSTRY, S3 -SERVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e6158-8211-476c-af08-6a66cf6adec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOOP THROUGH ALL DISTRICT\n",
    "#for dis_num in UST_district:\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "DICT = {\"SFH\":1110.0, \"DFH\":1121.0, \"MFH\":1122.0, \"HABITAT_COMMUNAUTAIRE\":1130.0, \"HOTEL\":1211.0, \"HEBERGEMENT\":1212.0, \"OFFICE\":1220.0, \"COMMERCIAL\":1230.0, \"RESTO_BAR\":1231.0, \"TRANSP_STATIONS\":1241.0, \"GARAGE\": 1242.0, \n",
    "        \"INDUSTRIAL\":1251.0, \"RESERVOIRS\":1252.0, \"CULTURAL\":1261.0, \"MUSEUM_LIBRARY\":1262.0, \"ACADEMIC\":1263.0, \"HOSPITAL\":1264.0, \"SPORTS\":1265.0, \"AGRICULTURE\":1271.0, \"RELIGIOUS\":1272.0, \"ANCIENT\":1273.0, \"OTHER_PUBLIC\":1274.0, \"OUTSIDE\": 1275.0,\n",
    "        \"ANIMALS\":1276.0, \"GREENHOUSE\":1277.0, \"AGRI_STORAGE\":1278.0}\n",
    "\n",
    "RES_code = itemgetter(\"SFH\", \"DFH\", \"MFH\")(DICT)\n",
    "AGR_code = itemgetter(\"AGRICULTURE\")(DICT) #, \"ANIMALS\", \"GREENHOUSE\", \"AGRI_STORAGE\"\n",
    "IND_code = itemgetter(\"INDUSTRIAL\")(DICT)\n",
    "SER_code = itemgetter(\"HABITAT_COMMUNAUTAIRE\",\"HOTEL\", \"HEBERGEMENT\", \"OFFICE\", \"COMMERCIAL\", \"RESTO_BAR\", \"TRANSP_STATIONS\",\"CULTURAL\", \"MUSEUM_LIBRARY\", \"ACADEMIC\", \"HOSPITAL\", \"SPORTS\")(DICT)\n",
    "OTHER_code = itemgetter(\"GARAGE\",\"RESERVOIRS\", \"ANCIENT\", \"OUTSIDE\", \"RELIGIOUS\",\"OTHER_PUBLIC\")(DICT)\n",
    "\n",
    "# SINGLE DISTRICT\n",
    "dis_num = LUGANO_UST\n",
    "DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "dis_filename = DISTRICT + \".csv\"\n",
    "dis_csv = CANTON_CSV_DIRECTORY / dis_filename\n",
    "dis_rea = pd.read_csv(dis_csv, sep= \";\", header=0, index_col=0).set_index(\"EGID\")\n",
    "dis_rea = dis_rea.drop([\"index.1\"], axis=1)\n",
    "\n",
    "dr = dis_rea.drop([\"GABBJ\",\"GSTAT\", \"GBAUJ\", \"GBAUP\",\"GWAERZW1\",\"GENW1\",\"GWAERDATW1\", \"GWAERDATH1\", \"GENH1\", \"GWAERZH1\",\"GWAERZW2\",\"GENW2\",\"GWAERDATW2\", \"GWAERDATH2\", \"GENH2\", \"GWAERZH2\"], axis=1) #\n",
    "dr = dr.reset_index()\n",
    "#dr.dtypes\n",
    "\n",
    "RES = dr.loc[dr[\"GKLAS_fill\"].isin(RES_code)]\n",
    "AGR = dr.loc[dr[\"GKLAS_fill\"] == (AGR_code)]\n",
    "IND = dr.loc[dr[\"GKLAS_fill\"] == (IND_code)]\n",
    "SER = dr.loc[dr[\"GKLAS_fill\"].isin(SER_code)]\n",
    "OTHER = dr.loc[dr[\"GKLAS_fill\"].isin(OTHER_code)]\n",
    "\n",
    "print(DISTRICT, \"RES:\", RES[\"EGID\"].count() , \"AGR:\", AGR[\"EGID\"].count(), \"IND:\", IND[\"EGID\"].count() , \"SER:\", SER[\"EGID\"].count(), \"OTHER:\", OTHER[\"EGID\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56d873-2602-4759-a013-bdb13b438285",
   "metadata": {},
   "source": [
    "### RESIDENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d40b9f-9f56-4d5d-af34-de54c3ec9f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PER COMUNE -  Getting the number of households according to ERA (EWID) -> matches pretty well with this layer: https://map.geo.admin.ch/?layers=ch.are.wohnungsinventar-zweitwohnungsanteil\n",
    "import ast\n",
    "\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "\n",
    "#for com_num in communes:\n",
    "com_num = 5269    \n",
    "print(\"comune: \", com_num)\n",
    "fileloc =  CANTON_CSV_DIRECTORY\n",
    "# reading comune raw gdf file\n",
    "filename = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "data_res = pd.read_csv(fileloc/filename , header=0, index_col=0)\n",
    "data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "\n",
    "data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "data_res = data_res.reset_index()\n",
    "\n",
    "for g in range(len(data_res)):\n",
    "    if (data_res.at[g,\"gkat\"] == 1020):\n",
    "        data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "    elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "        data_res.loc[g,\"gklas_fill\"] = 1122 # 63% of known 1040\n",
    "        \n",
    "data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "\n",
    "for g in range(len(data_res)):\n",
    "    if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "        if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "            data_res.at[g,\"ewid\"] = \"['1']\"\n",
    "        elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "            data_res.at[g,\"ewid\"] = \"['1', '2']\"\n",
    "        elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "            data_res.at[g,\"ewid\"] = \"['1', '2', '3', '4', '5', '6', '7']\" # GET real median\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for g in range(len(data_res)):\n",
    "    data_res.at[g,\"ewid\"] = ast.literal_eval(data_res.at[g,\"ewid\"])      \n",
    "\n",
    "data_res[\"ewid_len\"] = data_res.ewid.apply(lambda x: len(x))\n",
    "\n",
    "test = data_res[[\"gklas_fill\",\"ewid\",\"ewid_len\"]]\n",
    "\n",
    "print(test[\"ewid_len\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c129eb3-4de4-49b0-9284-ffbf9ae581a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## PER COMUNE\n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "b = district_res_elec.loc[district_res_elec[\"ggdenr\"] == 5009]# GGDENR Numéro OFS de la commune\n",
    "b = b[REA_CODES_DESIRED_LC]\n",
    "b = b.reset_index()\n",
    "b.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e1e26-117c-4e90-9d35-c0b8d9539e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AGE GROUPBY - COUNT\n",
    "b = district_res_elec.loc[district_res_elec[\"ggdenr\"] == 5009]# GGDENR Numéro OFS de la commune\n",
    "\n",
    "GKAT_group = b[[\"gkat\", \"gklas\", \"ewid\"]]\n",
    "GKAT_group = GKAT_group.groupby([\"gkat\", \"gklas\"]).count()\n",
    "GKAT_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfeb3b6-45be-4f4f-8f04-bb87a6d436c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1/3) Calculating number of households and ppl, and elec consumption - DOESN'tGIVE REASONABLE VALUES\n",
    "for dis_num in UST_district:\n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    dis_filename = DISTRICT + \".csv\"\n",
    "    dis_csv = CANTON_CSV_DIRECTORY / dis_filename\n",
    "    dis_rea = pd.read_csv(dis_csv, sep= \";\", header=0, index_col=0).set_index(\"EGID\")\n",
    "    dis_rea = dis_rea.drop([\"index.1\"], axis=1)\n",
    "    dr = dis_rea.drop([\"GABBJ\",\"GSTAT\", \"GBAUJ\", \"GBAUP\",\"GWAERZW1\",\"GENW1\",\"GWAERDATW1\", \"GWAERDATH1\", \"GENH1\", \"GWAERZH1\",\"GWAERZW2\",\"GENW2\",\"GWAERDATW2\", \"GWAERDATH2\", \"GENH2\", \"GWAERZH2\"], axis=1) #\n",
    "    dr = dr.reset_index()\n",
    "    \n",
    "    dr[\"ppl\"] = 2.4\n",
    "    dr[\"elec\"] = SFH_4pp_elec\n",
    "    dr[\"num_hh\"] = 1\n",
    "\n",
    "for g in range(len(dr)):\n",
    "    \n",
    "    ### RESIDENTIAL\n",
    "    if dr.at[g,\"GKLAS_fill\"] == 1110:\n",
    "        dr.at[g,\"ppl\"] = min(0.93*SFH_PP,(dr.at[g,\"GAREA_fill\"]*dr.at[g,\"GASTW_fill\"]/AVG_M2_PP))\n",
    "        dr.at[g,\"elec\"] = SFH_4pp_elec + (dr.at[g,\"ppl\"]-4)*SFH_1pp_elec_diff\n",
    "        \n",
    "    elif dr.at[g,\"GKLAS_fill\"] == 1121:\n",
    "        dr.at[g,\"num_hh\"] = 2\n",
    "        dr.at[g,\"ppl\"] = dr.at[g,\"num_hh\"]*(min(1.1*MFH_PP,(dr.at[g,\"GAREA_fill\"]*dr.at[g,\"GASTW_fill\"]/dr.at[g,\"num_hh\"]/AVG_M2_PP)))\n",
    "        dr.at[g,\"elec\"] = SFH_4pp_elec + (dr.at[g,\"ppl\"]-4)*SFH_1pp_elec_diff\n",
    "    \n",
    "    elif dr.at[g,\"GKLAS_fill\"] == 1122:\n",
    "#        if (dr.at[g,\"GAREA_fill\"]*dr.at[g,\"GASTW_fill\"]) <SUPERFICIE_MEDIA:\n",
    "#            dr.at[g,\"num_hh\"] = dr.at[g,\"GASTW_fill\"]\n",
    "#        else:\n",
    "        dr.at[g,\"num_hh\"] = 0.6*(dr.at[g,\"GAREA_fill\"]*dr.at[g,\"GASTW_fill\"])/(1.5*SUPERFICIE_MEDIA)\n",
    "        dr.at[g,\"ppl\"] = dr.at[g,\"num_hh\"]*min(1.1*MFH_PP,(dr.at[g,\"GAREA_fill\"]*dr.at[g,\"GASTW_fill\"]/dr.at[g,\"num_hh\"]/AVG_M2_PP))\n",
    "        dr.at[g,\"elec\"] = MFH_2pp_elec + (dr.at[g,\"ppl\"]-2)*MFH_1pp_elec_diff    \n",
    "    \n",
    "    ### SERVICES\n",
    "    else:\n",
    "        dr.at[g,\"ppl\"] = 0\n",
    "        dr.at[g,\"elec\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcb07d-24a9-48d2-908e-59807924c38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2/3) Calculating number of households and ppl, and elec consumption - DOESN'tGIVE REASONABLE VALUES\n",
    "dr_SFH = dr.loc[dr[\"GKLAS_fill\"] == 1110]\n",
    "print(\"SFH_median: \",round(dr_SFH[\"ppl\"].median(),1), \"SFH_mean: \",round(dr_SFH[\"ppl\"].mean(),1))\n",
    "        \n",
    "dr_DFH = dr.loc[dr[\"GKLAS_fill\"] == 1121]\n",
    "print(\"DFH_median: \", round(dr_DFH[\"ppl\"].median()/dr_DFH[\"num_hh\"].median(),1), \"DFH_mean: \",round(dr_DFH[\"ppl\"].mean()/dr_DFH[\"num_hh\"].median(),1))\n",
    "\n",
    "dr_MFH = dr.loc[dr[\"GKLAS_fill\"] == 1122]\n",
    "print(\"MFH_median: \",round(dr_MFH[\"ppl\"].median()/dr_MFH[\"num_hh\"].median(),1), \"MFH_mean: \",round(dr_MFH[\"ppl\"].mean()/dr_MFH[\"num_hh\"].median(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264fc06-d3a9-43cc-b1be-b7234ff3d6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (3/3) Calculating number of households and ppl, and elec consumption - DOESN'tGIVE REASONABLE VALUES\n",
    "dr_hh = dr[\"num_hh\"].sum()\n",
    "dr_ppl = dr[\"ppl\"].sum()\n",
    "dr_elec = dr[\"elec\"].sum()\n",
    "print( DISTRICT,\" hh:\",dr_hh, \" ppl:\",dr_ppl, \" elec kWh:\", dr_elec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
