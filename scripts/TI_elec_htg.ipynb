{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0917ed-87a5-4602-be73-3cb6fba0d129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import pathlib\n",
    "import fiona\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from geopandas.tools import geocode\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from tqdm import trange, tqdm\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c2c62f-c11b-4918-96e3-96d5292e7178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the typical size of a MFH in each district - works\n",
    "\n",
    "def get_hh_MFH(DATA_DIRECTORY,CANTON_CSV_DIRECTORY,UST_district, REA_CODES_DESIRED_LC):\n",
    "\n",
    "    TI_hh = pd.DataFrame()\n",
    "    district_hh = pd.DataFrame()\n",
    "    ewid_count = pd.DataFrame()\n",
    "\n",
    "    for dis_num in UST_district:\n",
    "        communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"]\n",
    "\n",
    "        for com_num in communes:\n",
    "            #print(\"comune: \", com_num)\n",
    "\n",
    "            fileloc =  CANTON_CSV_DIRECTORY\n",
    "\n",
    "            filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "            filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "            if pathlib.Path(fileloc/filename_gdf).exists():\n",
    "                data_res = pd.read_csv(fileloc/filename_gdf , header=0, index_col=0)\n",
    "                #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "            elif pathlib.Path(fileloc/filename_data).exists():\n",
    "                data_res = pd.read_csv(fileloc/filename_data , header=0, index_col=0)\n",
    "                #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "            data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "            data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "            data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "\n",
    "            data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "            data_res = data_res.reset_index(drop=True)\n",
    "\n",
    "            for g in range(len(data_res)):\n",
    "                 if (np.isnan(data_res.at[g,\"gklas\"])):\n",
    "                    if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                        data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "                    elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                        data_res.loc[g,\"gklas_fill\"] = 1122\n",
    "\n",
    "            data_res[\"egid_duplicate\"] = data_res.duplicated(subset = [\"egid\"], keep = False)\n",
    "\n",
    "            no_nan = data_res.dropna(subset = [\"ewid\"])\n",
    "            no_nan = no_nan.reset_index(drop=True) #,, inplace=True\n",
    "\n",
    "            for g in range(len(no_nan)):\n",
    "                ewid_count.at[g,\"ewid\"] = ast.literal_eval(no_nan.at[g,\"ewid\"]) \n",
    "\n",
    "            no_nan[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "\n",
    "            duplicates = no_nan.loc[no_nan[\"egid_duplicate\"] == True]\n",
    "            group = duplicates.groupby(\"egid\")[\"ewid_len\"].sum()\n",
    "            group = pd.DataFrame(group)\n",
    "\n",
    "            # Replace ewid_len values in DataFrame A based on DataFrame B\n",
    "            no_nan['ewid_len'] = no_nan.apply(lambda row: group.loc[row['egid'], 'ewid_len'] if row['egid'] in group.index else row['ewid_len'], axis=1)\n",
    "            no_nan.drop_duplicates(subset =[\"egid\"], keep=\"first\", inplace = True)\n",
    "\n",
    "            district_hh = pd.concat([district_hh,no_nan])\n",
    "\n",
    "        abi_num = district_hh [[\"egid\",\"gklas_fill\",\"ewid_len\"]]\n",
    "        abi_num = abi_num.groupby([\"gklas_fill\",\"ewid_len\"]).count()\n",
    "        abi_num = abi_num.rename(columns={\"egid\":\"count\"})\n",
    "        abi_num.rename_axis(index={\"gklas_fill\":\"gklas\",\"ewid_len\":\"number of households\"}, inplace = True)\n",
    "        med = pd.DataFrame(abi_num).reset_index(level=\"number of households\")\n",
    "        med[\"TOT\"] = med[\"number of households\"]*med[\"count\"]\n",
    "        MFH = med.loc[1122.0]\n",
    "        MFH_avghh = MFH[\"TOT\"].sum()/MFH[\"count\"].sum()\n",
    "\n",
    "        DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "        hh = pd.DataFrame([MFH_avghh], index=[\"MFH_avg_households\"], columns=[DISTRICT])\n",
    "\n",
    "        TI_hh = pd.concat([TI_hh,hh], axis=1)\n",
    "        district_hh = district_hh[0:0]\n",
    "\n",
    "    TI_hh[\"TI\"] = TI_hh.mean(axis=1)\n",
    "    return TI_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7aaf02b-2376-455a-b660-2089245d69b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_number_list(num):\n",
    "    number_list = [str(i) for i in range(1, num + 1)]\n",
    "    return number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c2fd6e-9417-4033-a911-a8935f1959b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ration between primaty and secondary households - works\n",
    "def get_hh_stats(DATA_DIRECTORY,CANTON_CSV_DIRECTORY,UST_district):\n",
    "\n",
    "    sup_file = DATA_DIRECTORY / \"support_data\" \n",
    "\n",
    "    data_households_2020 = pd.read_excel(sup_file/\"household_inventory_2020-10_2056.xlsx\", sheet_name=\"ZWG_2020_Q3\" , header=0, index_col=0)\n",
    "    data_households_2023 = pd.read_excel(sup_file/\"household_inventory_2023-03_2056.xlsx\", sheet_name=\"ZWG_2023\" , header=0, index_col=0)\n",
    "    \n",
    "    ## GET ratios for primary households and difference between 2023 and 2020 - WORKS!\n",
    "\n",
    "    TI_hh_2020 = pd.DataFrame()\n",
    "    district_hh_2020 = pd.DataFrame()\n",
    "    district_hh_2023 = pd.DataFrame()\n",
    "\n",
    "    for dis_num in UST_district:\n",
    "        communes = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"]    \n",
    "        DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "\n",
    "        for com_name in communes:\n",
    "            hh_2020 = data_households_2020.loc[data_households_2020[\"Name\"] == com_name]#\n",
    "            hh_2020 = hh_2020.reset_index()\n",
    "\n",
    "            hh_2023 = data_households_2023.loc[data_households_2023[\"Name\"] == com_name]#\n",
    "            hh_2023 = hh_2023.reset_index()\n",
    "\n",
    "            district_hh_2020 = pd.concat([district_hh_2020, hh_2020])\n",
    "            district_hh_2023 = pd.concat([district_hh_2023, hh_2023])\n",
    "\n",
    "        dis_tot_hh_2020 = district_hh_2020[\"ZWG_3150\"].sum()\n",
    "        dis_tot_primary_2020 = district_hh_2020[\"ZWG_3010\"].sum() + district_hh_2020[\"ZWG_3100\"].sum()\n",
    "\n",
    "        calc_dist_secondary_2020 = round(((dis_tot_hh_2020 - dis_tot_primary_2020)/dis_tot_hh_2020),3)\n",
    "\n",
    "        ratio_secondary_mean_2020 = district_hh_2020[\"ZWG_3120\"].mean()\n",
    "        ratio_secondary_median_2020 = district_hh_2020[\"ZWG_3120\"].median()\n",
    "\n",
    "        dis_tot_hh_2023 = district_hh_2023[\"ZWG_3150\"].sum()\n",
    "        dis_tot_primary_2023 = district_hh_2023[\"ZWG_3010\"].sum() + district_hh_2023[\"ZWG_3100\"].sum()\n",
    "\n",
    "        hh_diff = round((dis_tot_hh_2020/dis_tot_hh_2023),3)\n",
    "\n",
    "        d_hh = pd.DataFrame([calc_dist_secondary_2020, hh_diff], index=[\"hh2020_p_ratio\", \"hh2023_diff\"], columns=[DISTRICT])\n",
    "\n",
    "        TI_hh_2020 = pd.concat([TI_hh_2020,d_hh], axis=1)\n",
    "        district_hh_2020 = district_hh_2020[0:0]\n",
    "        district_hh_2023 = district_hh_2023[0:0]\n",
    "\n",
    "    return TI_hh_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fba719-146e-49be-a841-d61b9f440c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pop(communes):\n",
    "    com_pop = data_pop[data_pop[\"Num_Comune\"].isin(communes)]# GGDENR Num√©ro OFS de la commune district_limits[district_limits['NAME'].isin(TI_districts)]\n",
    "    com_pop = com_pop.reset_index(drop =True)\n",
    "    d_pop = com_pop[\"Stato della popolazione al 31 dicembre\"].sum()\n",
    "    com_pop[\"percent of disrict pop\"] = round((com_pop[\"Stato della popolazione al 31 dicembre\"]/d_pop),2)\n",
    "    \n",
    "    return com_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23db6f0-fb4b-4c13-85fe-5b7abee6a681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coordinates(comune):\n",
    "    base_url = \"https://api3.geo.admin.ch/rest/services/api/SearchServer\"\n",
    "    params = {\n",
    "        \"searchText\": comune,\n",
    "        \"type\": \"locations\",\n",
    "        \"lang\": \"en\",\n",
    "        \"limit\": 1,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Check for any request errors\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract longitude and latitude from the first result\n",
    "        if \"results\" in data and len(data[\"results\"]) > 0:\n",
    "            result = data[\"results\"][0]\n",
    "            longitude = result[\"attrs\"][\"lon\"]\n",
    "            latitude = result[\"attrs\"][\"lat\"]\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            print(\"No results found for Lugano.\")\n",
    "            return None, None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c740137-d388-4f6f-99f3-b9d86a79c315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 1 - COMMON INFO\n",
    "\n",
    "## DEFINE CONSTANTS \n",
    "REA_CODES_DESIRED_LC = ['egid', 'strname_deinr', 'ggdename', 'ggdenr',\n",
    "       'gexpdat', 'gdekt', 'egrid','gebnr', 'gkode', 'gkodn', 'gksce', 'gstat', 'gkat', 'gklas',\n",
    "       'gbauj', 'gbaup', 'gabbj', 'garea', 'gastw', 'gazzi', 'gebf', 'gwaerzh1',\n",
    "       'genh1', 'gwaersceh1', 'gwaerdath1', 'gwaerzh2', 'genh2', 'gwaersceh2',\n",
    "       'gwaerdath2', 'gwaerzw1', 'genw1', 'gwaerscew1', 'gwaerdatw1',\n",
    "       'gwaerzw2', 'genw2', 'gwaerscew2', 'gwaerdatw2',\"ewid\"]\n",
    "\n",
    "EPSG_CODE = \"EPSG:2056\"\n",
    "GKLAS = [1110,1121,1122,1130, 1211, 1212, 1220, 1230, 1251, 1261, 1262, 1263, 1264, 1265, 1272, 1241, 1242, 1271, 1274, 1275, 1276, 1277, 1278, 1231, 1252, 1273]\n",
    "GKLAS_HTG = [1110, 1121, 1122, 1130, 1211, 1212, 1220, 1230, 1251, 1261, 1262, 1263, 1264, 1265, 1272, 1231]\n",
    "GKLAS_NOHTG = [1241, 1242, 1252, 1271, 1273, 1274, 1275, 1276, 1277, 1278]\n",
    "GKLAS_SFH = 1110\n",
    "GKLAS_DFH = 1121\n",
    "GKLAS_MFH = 1122\n",
    "GKLAS_RES = [GKLAS_SFH, GKLAS_DFH, GKLAS_MFH]\n",
    "GBAUP = [8011,8012,8013,8014,8015,8016,8017,8018,8019,8020,8021,8022,8023]\n",
    "GSTAT = 1004 # existing\n",
    "\n",
    "DICT = {\"SFH\":1110.0, \"DFH\":1121.0, \"MFH\":1122.0, \"HABITAT_COMMUNAUTAIRE\":1130.0, \"HOTEL\":1211.0, \"HEBERGEMENT\":1212.0, \"OFFICE\":1220.0, \"COMMERCIAL\":1230.0, \"RESTO_BAR\":1231.0, \"TRANSP_STATIONS\":1241.0, \"GARAGE\": 1242.0, \n",
    "        \"INDUSTRIAL\":1251.0, \"RESERVOIRS\":1252.0, \"CULTURAL\":1261.0, \"MUSEUM_LIBRARY\":1262.0, \"ACADEMIC\":1263.0, \"HOSPITAL\":1264.0, \"SPORTS\":1265.0, \"AGRICULTURE\":1271.0, \"RELIGIOUS\":1272.0, \"ANCIENT\":1273.0, \"OTHER_PUBLIC\":1274.0, \"OUTSIDE\": 1275.0,\n",
    "        \"ANIMALS\":1276.0, \"GREENHOUSE\":1277.0, \"AGRI_STORAGE\":1278.0}\n",
    "\n",
    "RES_code = itemgetter(\"SFH\", \"DFH\", \"MFH\")(DICT)\n",
    "AGR_code = itemgetter(\"AGRICULTURE\")(DICT) #, \"ANIMALS\", \"GREENHOUSE\", \"AGRI_STORAGE\"\n",
    "IND_code = itemgetter(\"INDUSTRIAL\")(DICT)\n",
    "SER_code = itemgetter(\"HABITAT_COMMUNAUTAIRE\",\"HOTEL\", \"HEBERGEMENT\", \"OFFICE\", \"COMMERCIAL\", \"RESTO_BAR\", \"TRANSP_STATIONS\",\"CULTURAL\", \"MUSEUM_LIBRARY\", \"ACADEMIC\", \"HOSPITAL\", \"SPORTS\",\"RELIGIOUS\")(DICT)\n",
    "OTHER_code = itemgetter(\"GARAGE\",\"RESERVOIRS\", \"ANCIENT\", \"OUTSIDE\", \"OTHER_PUBLIC\")(DICT)\n",
    "\n",
    "CANTON = \"Ticino\"\n",
    "\n",
    "### Consumi di energia (in gigawattora), secondo la destinazione e il vettore energetico, in Ticino, nel 2020\n",
    "S1_AGR_elec = 61640 # kWh/y = consumo \"Altro (cantieri, agricoltura ecc.)\" 2020 / Aziende primario 2020\n",
    "S2_IND_elec = 153650 #kWh/y consumo \"Artigianato e industria\"/ Aziende secondario 2020\n",
    "S3_SER_elec = 22180 #kWh/y consumo \"Commercio e servizi\" /Aziende terzario 2020\n",
    "AVG_HH_ELEC_TI = 3499 # kWh/y USTAT 2020 = tot apparecchi/tot abitazioni ewid \n",
    "\n",
    "\n",
    "AVG_DHW_PP = 40 # L/DAY/PERSON ODIS\n",
    "AVG_M2_PP = 49.1 #TI - Superficie media per occupante, per Cantone, 2021, https://www.bfs.admin.ch/bfs/it/home/statistiche/costruzioni-abitazioni/abitazioni/condizioni-abitazione/superficie-persona.html\n",
    "SUPERFICIE_MEDIA = 97.8 #https://www.bfs.admin.ch/bfs/it/home/statistiche/costruzioni-abitazioni/abitazioni/dimensioni.html\n",
    "MFH_PP = 2.1 #https://www.bfs.admin.ch/bfs/it/home/statistiche/costruzioni-abitazioni/abitazioni/condizioni-abitazione/densita-utilizzazione.html\n",
    "SFH_PP = 2.7\n",
    "NET_ERA_SHARE = 0.9 # SIA 416 - la superficie netta di un locale o un gruppo di locali corrisponde ca. al 90% della superficie del piano [SIA 380-4 (2006)]\n",
    "UNOCC = 0.0283 # should be around 0.0283 for TI\n",
    "\n",
    "## SET LOCATION OF FILES\n",
    "pathlib.Path()\n",
    "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
    "p = NOTEBOOK_PATH.parent\n",
    "DATA_DIRECTORY = p / \"data\"\n",
    "MAP_DIRECTORY = DATA_DIRECTORY /'maps'\n",
    "BLDG_DIRECTORY = DATA_DIRECTORY /'Buildings_rea'\n",
    "CANTON = \"Ticino\"\n",
    "CANTON_CSV_DIRECTORY = BLDG_DIRECTORY / CANTON\n",
    "sup_file = DATA_DIRECTORY / \"support_data\" \n",
    "\n",
    "## READ SUPPORT DATA\n",
    "data_sh = pd.read_csv(sup_file/\"demand_SH.csv\" , header=0, index_col=0)\n",
    "data_dhw = pd.read_csv(sup_file/\"demand_DHW.csv\", header=0, index_col=0)\n",
    "fehh = pd.read_csv(sup_file/\"fehh.csv\", header=0, index_col=0)\n",
    "HDD =  pd.read_csv(sup_file/\"district_pop_weighted_HDD.csv\", sep=\";\", header=0, index_col=0)\n",
    "data_s123 = pd.read_csv(sup_file/\"S1_S2_S3.csv\" , header=0, index_col=0)\n",
    "# Read the shp file and decode the Geopandas dataframe using the Swiss coordinates (epsg code: 2056)\n",
    "SWISSTOPO_DISTRICT_FILE = \"swissboundaries3d_2023-01_2056_5728.shp/swissBOUNDARIES3D_1_4_TLM_BEZIRKSGEBIET.shp\" #shapefile downloaded from https://www.swisstopo.admin.ch/fr/geodata/landscape/boundaries3d.html\n",
    "SWISSTOPO_CANTON_FILE = \"swissboundaries3d_2023-01_2056_5728.shp/swissBOUNDARIES3D_1_4_TLM_KANTONSGEBIET.shp\" #shapefile downloaded from https://www.swisstopo.admin.ch/fr/geodata/landscape/boundaries3d.html. NOTE: The actual shapefile (.shp) is useless without the companion files: .dbf, .shx, .prj etc..\n",
    "\n",
    "UST_file = \"UST_comune.csv\"\n",
    "canton =\"TI\"\n",
    "UST_comune = pd.read_csv(MAP_DIRECTORY/UST_file)\n",
    "UST_comune = UST_comune[UST_comune['Cantone'] == canton].reset_index()\n",
    "UST_district = UST_comune['Numero del Distretto'].drop_duplicates().reset_index(drop=True)\n",
    "avg_hh_MFH = get_hh_MFH(DATA_DIRECTORY,CANTON_CSV_DIRECTORY,UST_district, REA_CODES_DESIRED_LC)\n",
    "hh_stats = get_hh_stats(DATA_DIRECTORY,CANTON_CSV_DIRECTORY,UST_district)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9fa347-507d-4624-b306-e57c0f8c6b1a",
   "metadata": {},
   "source": [
    "### AGR, IND, SER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f98e38-74ff-4e52-a7ba-7649b94ac254",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellinzona\n",
      "Blenio\n",
      "Leventina\n"
     ]
    }
   ],
   "source": [
    "# 4 - Calculate the heating and electricty demand for agriculture, industry, and services - WORKS!\n",
    "\n",
    "## CREATE EMPTY DATAFRAMES\n",
    "TI_nonres = pd.DataFrame()\n",
    "district_nonres = pd.DataFrame()\n",
    "S123 = pd.DataFrame()\n",
    "\n",
    "for dis_num in UST_district:  \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    print(DISTRICT)\n",
    "    \n",
    "    dis_S1_tot_elec = data_s123.loc[DISTRICT, \"S1_Aziende\"]*S1_AGR_elec\n",
    "    dis_S2_tot_elec = data_s123.loc[DISTRICT, \"S2_Aziende\"]*S2_IND_elec\n",
    "    dis_S3_tot_elec = data_s123.loc[DISTRICT, \"S3_Aziende\"]*S3_SER_elec\n",
    "    tot_elec = pd.DataFrame(np.array([[dis_S1_tot_elec], [dis_S2_tot_elec], [dis_S3_tot_elec]]),  columns = [DISTRICT], index = [ \"S1_agr_elec\", \"S2_ind_elec\", \"S3_ser_elec\"])\n",
    "    S123 = pd.concat([S123,tot_elec], axis = 1)\n",
    " \n",
    "    ic = len(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num)])\n",
    "\n",
    "\n",
    "    for com_i in range(ic):\n",
    "        com_name = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"].iloc[com_i] \n",
    "        com_num = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"].iloc[com_i]  \n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(CANTON_CSV_DIRECTORY/filename_gdf).exists():\n",
    "            data_nonres = pd.read_csv(CANTON_CSV_DIRECTORY/filename_gdf , header=0, index_col=0)\n",
    "            #data_rea = data_rea_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(CANTON_CSV_DIRECTORY/filename_data).exists():\n",
    "            data_nonres = pd.read_csv(CANTON_CSV_DIRECTORY/filename_data , header=0, index_col=0)\n",
    "            #data_rea = data_rea_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        # Setting up the base year to 2020, so drop buildings that where buit after 2020 \n",
    "        data_nonres = data_nonres.drop(data_nonres[(data_nonres[\"gbauj\"]==2021)|(data_nonres[\"gbauj\"]==2022)|(data_nonres[\"gbauj\"]==2022)].index)\n",
    "\n",
    "        # Keep only existing residential buildings\n",
    "        data_nonres = data_nonres.loc[data_nonres[\"gstat\"] == 1004]\n",
    "        data_nonres = data_nonres[REA_CODES_DESIRED_LC]\n",
    "        data_nonres = data_nonres.loc[(data_nonres[\"gkat\"] == 1040) | (data_nonres[\"gkat\"] == 1060) | (data_nonres[\"gkat\"] == 1080)]\n",
    "\n",
    "        data_nonres[\"garea_fill\"] = data_nonres[\"garea\"]\n",
    "        data_nonres[\"gbaup_fill\"] = data_nonres[\"gbaup\"]\n",
    "        data_nonres[\"gastw_fill\"] = data_nonres[\"gastw\"]\n",
    "        data_nonres[\"gklas_fill\"] =  data_nonres[\"gklas\"]\n",
    "\n",
    "        data_nonres = data_nonres.reset_index()\n",
    "\n",
    "        age_group = data_nonres[[\"gkat\", \"gklas\", \"gbaup\"]]\n",
    "        age_pivot = pd.pivot_table(age_group, values='gbaup', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        age_pivot = age_pivot.round(0)\n",
    "        age_col = list(age_pivot.columns)\n",
    "        for code in GKLAS:\n",
    "            if code in age_col:\n",
    "                pass\n",
    "            else:                \n",
    "                age_pivot[code] = 8016.0\n",
    "\n",
    "        size_group = data_nonres[[\"gkat\", \"gklas\", \"garea\"]]\n",
    "        size_pivot = pd.pivot_table(size_group, values='garea', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        size_pivot = size_pivot.round(0)\n",
    "        size_col = list(size_pivot.columns)\n",
    "\n",
    "        for g in range(len(data_nonres)):\n",
    "            if np.isnan(data_nonres.at[g,\"gklas\"]): \n",
    "                if (data_nonres.at[g,\"gkat\"] == 1040.0):\n",
    "                    data_nonres.loc[g,\"gklas_fill\"] = 1211.0 #\n",
    "                elif (data_nonres.at[g,\"gkat\"] == 1060.0):\n",
    "                    data_nonres.loc[g,\"gklas_fill\"] =1271.0 #\n",
    "                elif (data_nonres.at[g,\"gkat\"] == 1080.0):\n",
    "                    data_nonres.loc[g,\"gklas_fill\"] =1273.0 # \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if np.isnan(data_nonres.at[g,\"gbaup\"]):             \n",
    "                data_nonres.at[g,\"gbaup_fill\"] = age_pivot.loc[data_nonres.at[g,\"gkat\"],data_nonres.at[g,\"gklas_fill\"]]\n",
    "\n",
    "            if np.isnan(data_nonres.at[g,\"garea\"]):\n",
    "                if data_nonres.at[g,\"gklas_fill\"] in GKLAS_NOHTG:\n",
    "                    data_nonres.at[g,\"garea_fill\"] = 0\n",
    "                else:    \n",
    "                    data_nonres.at[g,\"garea_fill\"] = size_pivot.loc[(data_nonres.at[g,\"gkat\"],data_nonres.at[g,\"gklas_fill\"])]\n",
    "\n",
    "\n",
    "        data_nonres[\"garea_fill\"] = data_nonres[\"garea_fill\"].astype(np.int64)\n",
    "        data_nonres[\"gklas_fill\"] = data_nonres[\"gklas_fill\"].astype(np.int64)\n",
    "        data_nonres[\"gbaup_fill\"] = data_nonres[\"gbaup_fill\"].astype(np.int64)\n",
    "        data_nonres[\"gastw_fill\"] = data_nonres[\"gastw_fill\"].fillna(value = 1)\n",
    "\n",
    "\n",
    "        #drop duplicate egids\n",
    "        data_nonres = data_nonres.drop_duplicates(subset=['egid'], ignore_index = True)\n",
    "\n",
    "\n",
    "        for g in range(len(data_nonres)):\n",
    "            data_nonres.at[g,'k_SH'] = data_sh.loc[(data_nonres.at[g,\"gbaup_fill\"],str(data_nonres.at[g,\"gklas_fill\"]))]\n",
    "            data_nonres.at[g,'k_DHW'] = data_dhw.loc[(data_nonres.at[g,\"gbaup_fill\"],str(data_nonres.at[g,\"gklas_fill\"]))]\n",
    "            data_nonres.at[g,'k_fehh'] = fehh.loc[(data_nonres.at[g,\"gbaup_fill\"],str(data_nonres.at[g,\"gklas_fill\"]))]\n",
    "\n",
    "        district_nonres = pd.concat([district_nonres,data_nonres],ignore_index=True)\n",
    "\n",
    "    # calculating building attributes and demand\n",
    "    district_nonres['ERA'] = district_nonres['garea_fill']*district_nonres['gastw_fill']*NET_ERA_SHARE\n",
    "\n",
    "    district_nonres['SH'] = district_nonres['k_SH']*district_nonres['ERA']*HDD.at[\"r_HDD\",DISTRICT]        \n",
    "    district_nonres['DHW'] = district_nonres['k_DHW']*district_nonres['ERA']\n",
    "\n",
    "    agr_data = district_nonres.loc[district_nonres[\"gklas_fill\"] == (AGR_code)]\n",
    "    agr_htg = agr_data[\"SH\"].sum()\n",
    "    agr_dhw = agr_data[\"DHW\"].sum() \n",
    "\n",
    "    ind_data = district_nonres.loc[district_nonres[\"gklas_fill\"] == (IND_code)]\n",
    "    ind_htg = ind_data[\"SH\"].sum()\n",
    "    ind_dhw = ind_data[\"DHW\"].sum() \n",
    "\n",
    "    ser_data = district_nonres.loc[district_nonres[\"gklas_fill\"].isin(SER_code)]\n",
    "    ser_htg = ser_data[\"SH\"].sum()\n",
    "    ser_dhw = ser_data[\"DHW\"].sum()\n",
    "\n",
    "    d_nonres = pd.DataFrame([agr_htg, agr_dhw, ind_htg, ind_dhw, ser_htg, ser_dhw], index=[\"agr_htg\", \"agr_dhw\", \"ind_htg\", \"ind_dhw\", \"ser_htg\", \"ser_dhw\"], columns=[DISTRICT])\n",
    "    TI_nonres = pd.concat([TI_nonres,d_nonres], axis=1)\n",
    "    \n",
    "    district_nonres = district_nonres[0:0]\n",
    "    \n",
    "TI_nonres = pd.concat([TI_nonres,S123])\n",
    "TI_nonres[\"TI\"] = TI_nonres.sum(axis=1)\n",
    "\n",
    "path = str(DATA_DIRECTORY)+ \"/results/non_residential_htg_demand.csv\"\n",
    "filepath = pathlib.Path(path)  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "TI_nonres.to_csv(filepath, sep=\";\", encoding='utf-8-sig', index_label='index')\n",
    "print(TI_nonres)\n",
    "TI_nonres = TI_nonres[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e9070af-ee10-4410-981a-89414ccfe962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellinzona\n",
      "Blenio\n",
      "Leventina\n",
      "Locarno\n",
      "Lugano\n",
      "Mendrisio\n",
      "Riviera\n",
      "Vallemaggia\n",
      "           Bellinzona        Blenio     Leventina       Locarno        Lugano   \n",
      "agr_htg  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \\\n",
      "agr_dhw  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "ind_htg  5.431119e+07  1.237361e+06  3.609950e+07  2.780999e+07  6.019938e+07   \n",
      "ind_dhw  4.626261e+05  8.125200e+03  2.366739e+05  3.720699e+05  7.283565e+05   \n",
      "ser_htg  9.808703e+07  1.518085e+07  2.545904e+07  9.281103e+07  2.724004e+08   \n",
      "ser_dhw  1.079349e+07  2.038581e+06  3.093638e+06  2.045746e+07  4.222838e+07   \n",
      "\n",
      "            Mendrisio       Riviera   Vallemaggia            TI  \n",
      "agr_htg  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "agr_dhw  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  \n",
      "ind_htg  4.297798e+07  1.322245e+07  2.500217e+06  2.383581e+08  \n",
      "ind_dhw  5.835366e+05  8.593830e+04  2.272950e+04  2.500056e+06  \n",
      "ser_htg  6.877692e+07  1.472075e+07  1.131095e+07  5.987470e+08  \n",
      "ser_dhw  1.192304e+07  1.438517e+06  1.529852e+06  9.350296e+07  \n"
     ]
    }
   ],
   "source": [
    "# 4 - Calculate the heating demand for agriculture, industry, and services - WORKS!\n",
    "\n",
    "## CREATE EMPTY DATAFRAMES\n",
    "TI_nonres = pd.DataFrame()\n",
    "district_nonres = pd.DataFrame()\n",
    "\n",
    "for dis_num in UST_district:  \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    print(DISTRICT)\n",
    " \n",
    "    ic = len(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num)])\n",
    "\n",
    "    for com_i in range(ic):\n",
    "        com_name = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"].iloc[com_i] \n",
    "        com_num = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"].iloc[com_i]  \n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(CANTON_CSV_DIRECTORY/filename_gdf).exists():\n",
    "            data_nonres = pd.read_csv(CANTON_CSV_DIRECTORY/filename_gdf , header=0, index_col=0)\n",
    "            #data_rea = data_rea_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(CANTON_CSV_DIRECTORY/filename_data).exists():\n",
    "            data_nonres = pd.read_csv(CANTON_CSV_DIRECTORY/filename_data , header=0, index_col=0)\n",
    "            #data_rea = data_rea_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        # Setting up the base year to 2020, so drop buildings that where buit after 2020 \n",
    "        data_nonres = data_nonres.drop(data_nonres[(data_nonres[\"gbauj\"]==2021)|(data_nonres[\"gbauj\"]==2022)|(data_nonres[\"gbauj\"]==2022)].index)\n",
    "\n",
    "        # Keep only existing residential buildings\n",
    "        data_nonres = data_nonres.loc[data_nonres[\"gstat\"] == 1004]\n",
    "        data_nonres = data_nonres[REA_CODES_DESIRED_LC]\n",
    "        data_nonres = data_nonres.loc[(data_nonres[\"gkat\"] == 1040) | (data_nonres[\"gkat\"] == 1060) | (data_nonres[\"gkat\"] == 1080)]\n",
    "\n",
    "        data_nonres[\"garea_fill\"] = data_nonres[\"garea\"]\n",
    "        data_nonres[\"gbaup_fill\"] = data_nonres[\"gbaup\"]\n",
    "        data_nonres[\"gastw_fill\"] = data_nonres[\"gastw\"]\n",
    "        data_nonres[\"gklas_fill\"] =  data_nonres[\"gklas\"]\n",
    "\n",
    "        data_nonres = data_nonres.reset_index()\n",
    "\n",
    "        age_group = data_nonres[[\"gkat\", \"gklas\", \"gbaup\"]]\n",
    "        age_pivot = pd.pivot_table(age_group, values='gbaup', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        age_pivot = age_pivot.round(0)\n",
    "        age_col = list(age_pivot.columns)\n",
    "        for code in GKLAS:\n",
    "            if code in age_col:\n",
    "                pass\n",
    "            else:                \n",
    "                age_pivot[code] = 8016.0\n",
    "\n",
    "        size_group = data_nonres[[\"gkat\", \"gklas\", \"garea\"]]\n",
    "        size_pivot = pd.pivot_table(size_group, values='garea', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        size_pivot = size_pivot.round(0)\n",
    "        size_col = list(size_pivot.columns)\n",
    "\n",
    "        for g in range(len(data_nonres)):\n",
    "            if np.isnan(data_nonres.at[g,\"gklas\"]): \n",
    "                if (data_nonres.at[g,\"gkat\"] == 1040.0):\n",
    "                    data_nonres.loc[g,\"gklas_fill\"] = 1211.0 #\n",
    "                elif (data_nonres.at[g,\"gkat\"] == 1060.0):\n",
    "                    data_nonres.loc[g,\"gklas_fill\"] =1271.0 #\n",
    "                elif (data_nonres.at[g,\"gkat\"] == 1080.0):\n",
    "                    data_nonres.loc[g,\"gklas_fill\"] =1273.0 # \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if np.isnan(data_nonres.at[g,\"gbaup\"]):             \n",
    "                data_nonres.at[g,\"gbaup_fill\"] = age_pivot.loc[data_nonres.at[g,\"gkat\"],data_nonres.at[g,\"gklas_fill\"]]\n",
    "\n",
    "            if np.isnan(data_nonres.at[g,\"garea\"]):\n",
    "                if data_nonres.at[g,\"gklas_fill\"] in GKLAS_NOHTG:\n",
    "                    data_nonres.at[g,\"garea_fill\"] = 0\n",
    "                else:    \n",
    "                    data_nonres.at[g,\"garea_fill\"] = size_pivot.loc[(data_nonres.at[g,\"gkat\"],data_nonres.at[g,\"gklas_fill\"])]\n",
    "\n",
    "\n",
    "        data_nonres[\"garea_fill\"] = data_nonres[\"garea_fill\"].astype(np.int64)\n",
    "        data_nonres[\"gklas_fill\"] = data_nonres[\"gklas_fill\"].astype(np.int64)\n",
    "        data_nonres[\"gbaup_fill\"] = data_nonres[\"gbaup_fill\"].astype(np.int64)\n",
    "        data_nonres[\"gastw_fill\"] = data_nonres[\"gastw_fill\"].fillna(value = 1)\n",
    "\n",
    "\n",
    "        #drop duplicate egids\n",
    "        data_nonres = data_nonres.drop_duplicates(subset=['egid'], ignore_index = True)\n",
    "\n",
    "\n",
    "        for g in range(len(data_nonres)):\n",
    "            data_nonres.at[g,'k_SH'] = data_sh.loc[(data_nonres.at[g,\"gbaup_fill\"],str(data_nonres.at[g,\"gklas_fill\"]))]\n",
    "            data_nonres.at[g,'k_DHW'] = data_dhw.loc[(data_nonres.at[g,\"gbaup_fill\"],str(data_nonres.at[g,\"gklas_fill\"]))]\n",
    "            data_nonres.at[g,'k_fehh'] = fehh.loc[(data_nonres.at[g,\"gbaup_fill\"],str(data_nonres.at[g,\"gklas_fill\"]))]\n",
    "\n",
    "        district_nonres = pd.concat([district_nonres,data_nonres],ignore_index=True)\n",
    "\n",
    "    # calculating building attributes and demand\n",
    "    district_nonres['ERA'] = district_nonres['garea_fill']*district_nonres['gastw_fill']*NET_ERA_SHARE\n",
    "\n",
    "    district_nonres['SH'] = district_nonres['k_SH']*district_nonres['ERA']*HDD.at[\"r_HDD\",DISTRICT]        \n",
    "    district_nonres['DHW'] = district_nonres['k_DHW']*district_nonres['ERA']\n",
    "\n",
    "    agr_data = district_nonres.loc[district_nonres[\"gklas_fill\"] == (AGR_code)]\n",
    "    agr_htg = agr_data[\"SH\"].sum()\n",
    "    agr_dhw = agr_data[\"DHW\"].sum() \n",
    "\n",
    "    ind_data = district_nonres.loc[district_nonres[\"gklas_fill\"] == (IND_code)]\n",
    "    ind_htg = ind_data[\"SH\"].sum()\n",
    "    ind_dhw = ind_data[\"DHW\"].sum() \n",
    "\n",
    "    ser_data = district_nonres.loc[district_nonres[\"gklas_fill\"].isin(SER_code)]\n",
    "    ser_htg = ser_data[\"SH\"].sum()\n",
    "    ser_dhw = ser_data[\"DHW\"].sum()\n",
    "\n",
    "    d_nonres = pd.DataFrame([agr_htg, agr_dhw, ind_htg, ind_dhw, ser_htg, ser_dhw], index=[\"agr_htg\", \"agr_dhw\", \"ind_htg\", \"ind_dhw\", \"ser_htg\", \"ser_dhw\"], columns=[DISTRICT])\n",
    "    TI_nonres = pd.concat([TI_nonres,d_nonres], axis=1)\n",
    "    \n",
    "    district_nonres = district_nonres[0:0]\n",
    "    \n",
    "TI_nonres[\"TI\"] = TI_nonres.sum(axis=1)\n",
    "\n",
    "path = str(DATA_DIRECTORY)+ \"/results/non_residential_htg_demand.csv\"\n",
    "filepath = pathlib.Path(path)  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "TI_nonres.to_csv(filepath, sep=\";\", encoding='utf-8-sig', index_label='index')\n",
    "print(TI_nonres)\n",
    "TI_nonres = TI_nonres[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528011c6-ea75-4da1-a175-e22001f62521",
   "metadata": {},
   "source": [
    "### RESIDENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6fdc7a-da47-4c8e-8ad1-0cecd61faa20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellinzona\n",
      "Blenio\n",
      "Leventina\n",
      "Locarno\n",
      "Lugano\n",
      "Mendrisio\n",
      "Riviera\n",
      "Vallemaggia\n",
      "            Bellinzona        Blenio     Leventina       Locarno   \n",
      "hh        3.227000e+04  6.926000e+03  9.348000e+03  5.669300e+04  \\\n",
      "hh_p      2.710680e+04  2.576472e+03  4.440300e+03  3.248509e+04   \n",
      "hh_s      5.163200e+03  4.349528e+03  4.907700e+03  2.420791e+04   \n",
      "res_elec  9.945820e+07  1.278447e+07  1.991543e+07  1.344106e+08   \n",
      "res_sh    3.881450e+08  5.755207e+07  9.700613e+07  3.163994e+08   \n",
      "res_dhw   6.982699e+07  7.166092e+06  1.262213e+07  9.322201e+07   \n",
      "\n",
      "                Lugano     Mendrisio       Riviera   Vallemaggia            TI  \n",
      "hh        9.500600e+04  2.990200e+04  6.178000e+03  6.648000e+03  2.429710e+05  \n",
      "hh_p      7.714487e+04  2.466915e+04  5.047426e+03  2.645904e+03  1.761160e+05  \n",
      "hh_s      1.786113e+04  5.232850e+03  1.130574e+03  4.002096e+03  6.685499e+04  \n",
      "res_elec  2.861946e+08  9.104398e+07  1.853333e+07  1.278272e+07  6.751233e+08  \n",
      "res_sh    8.142494e+08  2.787720e+08  8.986406e+07  4.185786e+07  2.083846e+09  \n",
      "res_dhw   2.186806e+08  7.339828e+07  1.269510e+07  7.041705e+06  4.946529e+08  \n"
     ]
    }
   ],
   "source": [
    "#  5 - RESIDENTIAL ELECTRICITY AND HEAITNG DEMAND PER DISTRICT - Getting the number of households according to ERA (EWID) - WORKS!\n",
    "## NEED TO DOUBLE CHECK the gklass_fill argument\n",
    "\n",
    "## CREATE EMPTY DATAFRAMES\n",
    "TI_res = pd.DataFrame()\n",
    "district_res = pd.DataFrame()\n",
    "ewid_count = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dis_num in UST_district:  \n",
    "    DISTRICT = str(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num), \"Nome del Distretto\"].iloc[0])\n",
    "    print(DISTRICT)\n",
    "#    com_name = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"] \n",
    "#    com_num = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"] #\n",
    "    num = round(avg_hh_MFH.at[\"MFH_avg_households\",DISTRICT])\n",
    "\n",
    "    MFH_hh_list = generate_number_list(num)\n",
    "    MFH_avg = str(MFH_hh_list)\n",
    "    \n",
    "    ic = len(UST_comune.loc[(UST_comune[\"Numero del Distretto\"] == dis_num)])\n",
    "\n",
    "    for com_i in range(ic):\n",
    "        com_name = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Nome del Comune\"].iloc[com_i] \n",
    "        com_num = UST_comune.loc[UST_comune[\"Numero del Distretto\"] == dis_num, \"Numero UST del Comune\"].iloc[com_i]  \n",
    "        filename_gdf = \"raw-gdf-\"+str(com_num)+\".csv\"\n",
    "        filename_data = \"DATA_\"+str(com_num)+\".csv\"\n",
    "\n",
    "        if pathlib.Path(CANTON_CSV_DIRECTORY/filename_gdf).exists():\n",
    "            data_res = pd.read_csv(CANTON_CSV_DIRECTORY/filename_gdf , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "        elif pathlib.Path(CANTON_CSV_DIRECTORY/filename_data).exists():\n",
    "            data_res = pd.read_csv(CANTON_CSV_DIRECTORY/filename_data , header=0, index_col=0)\n",
    "            #data_res = data_res_dirty.drop_duplicates(keep=\"first\")\n",
    "\n",
    "        # Keep only existing residential buildings\n",
    "        data_res = data_res.loc[data_res[\"gstat\"] == 1004]\n",
    "        data_res = data_res.loc[(data_res[\"gkat\"] == 1020) | (data_res[\"gkat\"] == 1030)]\n",
    "        data_res = data_res[REA_CODES_DESIRED_LC]\n",
    "        \n",
    "        # Setting up the base year to 2020, so drop buildings that where buit after 2020 \n",
    "        data_res = data_res.drop(data_res[(data_res[\"gbauj\"]==2021)|(data_res[\"gbauj\"]==2022)|(data_res[\"gbauj\"]==2022)].index)\n",
    "\n",
    "        data_res[\"garea_fill\"] = data_res[\"garea\"]\n",
    "        data_res[\"gbaup_fill\"] = data_res[\"gbaup\"]\n",
    "        data_res[\"gastw_fill\"] = data_res[\"gastw\"]\n",
    "        data_res[\"gklas_fill\"] =  data_res[\"gklas\"]\n",
    "        \n",
    "        data_res = data_res.reset_index()\n",
    "\n",
    "        age_group = data_res[[\"gkat\", \"gklas\", \"gbaup\"]]\n",
    "        age_pivot = pd.pivot_table(age_group, values='gbaup', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        age_pivot = age_pivot.round(0)\n",
    "        age_col = list(age_pivot.columns)\n",
    "        for code in GKLAS:\n",
    "            if code in age_col:\n",
    "                pass\n",
    "            else:                \n",
    "                age_pivot[code] = 8016.0\n",
    "\n",
    "        size_group = data_res[[\"gkat\", \"gklas\", \"garea\"]]\n",
    "        size_pivot = pd.pivot_table(size_group, values='garea', index=['gkat'], columns=['gklas'], aggfunc=np.median)\n",
    "        size_pivot = size_pivot.round(0)\n",
    "        size_col = list(size_pivot.columns)\n",
    "        \n",
    "        for g in range(len(data_res)):\n",
    "            if np.isnan(data_res.at[g,\"gklas\"]): \n",
    "            ## CHECK IF THIS IS A MISTAKE AND IT IS CHANGING ALL VALUES NSTEAD OF MISSING ONES \n",
    "                if (data_res.at[g,\"gkat\"] == 1020):\n",
    "                    data_res.loc[g,\"gklas_fill\"] = 1110\n",
    "                elif (data_res.at[g,\"gkat\"] == 1030):\n",
    "                    data_res.loc[g,\"gklas_fill\"] = 1122 # 63% of known 1040\n",
    "\n",
    "            data_res[\"ewid\"] = data_res[\"ewid\"].fillna(value = \"['no']\")\n",
    "\n",
    "                    \n",
    "            if (data_res.at[g,\"ewid\"]==\"['no']\"):\n",
    "                if (data_res.at[g,\"gklas_fill\"] == 1110):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1121):\n",
    "                    data_res.at[g,\"ewid\"] = \"['1', '2']\"\n",
    "                elif (data_res.at[g,\"gklas_fill\"] == 1122):\n",
    "                    data_res.at[g,\"ewid\"] = MFH_avg # GET real median\n",
    "            else:\n",
    "                pass\n",
    "      \n",
    "            if np.isnan(data_res.at[g,\"gbaup\"]):             \n",
    "                data_res.at[g,\"gbaup_fill\"] = age_pivot.loc[data_res.at[g,\"gkat\"],data_res.at[g,\"gklas_fill\"]]\n",
    "\n",
    "            if np.isnan(data_res.at[g,\"garea\"]):\n",
    "                if data_res.at[g,\"gklas_fill\"] in GKLAS_NOHTG:\n",
    "                    data_res.at[g,\"garea_fill\"] = 0\n",
    "                else:    \n",
    "                    if data_res.at[g,\"gklas_fill\"] in size_col:\n",
    "                        pass\n",
    "                    else:   \n",
    "                        if data_res.at[g,\"gklas_fill\"] == 1130.0:\n",
    "                            size_pivot.loc[1040.0,1130.0] = size_pivot.loc[1040.0, 1211.0]\n",
    "                        else:\n",
    "                            pass\n",
    "                    data_res.at[g,\"garea_fill\"] = size_pivot.loc[(data_res.at[g,\"gkat\"],data_res.at[g,\"gklas_fill\"])]\n",
    "\n",
    "\n",
    "        data_res[\"garea_fill\"] = data_res[\"garea_fill\"].astype(np.int64)\n",
    "        data_res[\"gklas_fill\"] = data_res[\"gklas_fill\"].astype(np.int64)\n",
    "        data_res[\"gbaup_fill\"] = data_res[\"gbaup_fill\"].astype(np.int64)\n",
    "        data_res[\"gastw_fill\"] = data_res[\"gastw_fill\"].fillna(value = 1)\n",
    "        \n",
    "        for g in range(len(data_res)):\n",
    "            ewid_count.at[g,\"ewid\"] = ast.literal_eval(data_res.at[g,\"ewid\"])      \n",
    "\n",
    "        data_res[\"ewid_len\"] = ewid_count.ewid.apply(lambda x: len(x))\n",
    "        \n",
    "        # sum the ewid_len per egid\n",
    "        grouped = data_res.groupby('egid')[\"ewid_len\"].sum().reset_index()\n",
    "        grouped = pd.DataFrame(grouped)\n",
    "        \n",
    "        #drop duplicate egids\n",
    "        data_res = data_res.drop_duplicates(subset=['egid'])\n",
    "        \n",
    "        # Merge dataframes based on column 'egid'\n",
    "        data_res = data_res.merge(grouped, on='egid', how='left', suffixes=('_original', '_replacement'))\n",
    "\n",
    "        # Update 'ewid' values based on the replacement values\n",
    "        data_res['ewid_len'] = data_res['ewid_len_replacement'].fillna(data_res['ewid_len_original'])\n",
    "\n",
    "        # Drop the original and replacement columns\n",
    "        data_res = data_res.drop(columns=['ewid_len_original', 'ewid_len_replacement'])\n",
    "                \n",
    "        for g in range(len(data_res)):\n",
    "            data_res.at[g,'k_SH'] = data_sh.loc[(data_res.at[g,\"gbaup_fill\"],str(data_res.at[g,\"gklas_fill\"]))]\n",
    "            data_res.at[g,'k_DHW'] = data_dhw.loc[(data_res.at[g,\"gbaup_fill\"],str(data_res.at[g,\"gklas_fill\"]))]\n",
    "            data_res.at[g,'k_fehh'] = fehh.loc[(data_res.at[g,\"gbaup_fill\"],str(data_res.at[g,\"gklas_fill\"]))]\n",
    "\n",
    "        district_res = pd.concat([district_res,data_res],ignore_index=True)\n",
    "    \n",
    "    sample_size = int(len(district_res)*(1-UNOCC))\n",
    "\n",
    "    # Take a random sample of 90% of the rows\n",
    "    district_res_occ = district_res.sample(n=sample_size, random_state=1) #random state can be changed, otherwise always same sample\n",
    "    district_res_occ.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    d_hh = (district_res_occ[\"ewid_len\"].sum())\n",
    "    \n",
    "    #d_hh = d_hh*hh_stats.at[\"hh2023_diff\",DISTRICT] #\"hh2023_diff\" = ratio of residences in 2020/2023\n",
    "    d_hh_p = d_hh*(1-hh_stats.at[\"hh2020_p_ratio\",DISTRICT]) # (1-\"hh2020_p_ratio\"= primary residences \n",
    "    d_hh_s = d_hh*(hh_stats.at[\"hh2020_p_ratio\",DISTRICT])  # \"hh2020_p_ratio\" = ratio of secondary residences \n",
    "    \n",
    "    num_rows = len(district_res_occ)\n",
    "    num_ones = round(len(district_res_occ)*(1-hh_stats.at[\"hh2020_p_ratio\",DISTRICT]))\n",
    "\n",
    "    rand_values = np.array([1] * num_ones + [1/4] * (num_rows - num_ones))\n",
    "    np.random.shuffle(rand_values)\n",
    "\n",
    "    # Assign the generated random values to the \"primary\" column\n",
    "    district_res_occ['primary'] = rand_values\n",
    "\n",
    "    # calculating building attributes and demand\n",
    "    district_res_occ['ERA'] = district_res_occ['garea_fill']*district_res_occ['gastw_fill']*NET_ERA_SHARE\n",
    "    \n",
    "    for g in range(len(district_res_occ)):\n",
    "        if district_res_occ.at[g,'primary'] == 1:\n",
    "            district_res_occ.at[g,'SH'] = district_res_occ.at[g,'k_SH']*district_res_occ.at[g,'ERA']*district_res_occ.at[g,'primary']*HDD.at[\"r_HDD\",DISTRICT]\n",
    "        else:\n",
    "            district_res_occ.at[g,'SH'] = district_res_occ.at[g,'k_SH']*district_res_occ.at[g,'ERA']*(district_res_occ.at[g,'primary']/3)*HDD.at[\"r_HDD\",DISTRICT]\n",
    "            \n",
    "    district_res_occ['DHW'] = district_res_occ['k_DHW']*district_res_occ['ERA']*district_res_occ['primary']\n",
    "    district_res_occ['ELEC'] = district_res_occ['ewid_len']*AVG_HH_ELEC_TI*district_res_occ['primary']\n",
    "\n",
    "    \n",
    "    d_elec = district_res_occ[\"ELEC\"].sum() \n",
    "    d_sh = district_res_occ[\"SH\"].sum() \n",
    "    d_dhw = district_res_occ[\"DHW\"].sum() \n",
    "    \n",
    "    d = pd.DataFrame([d_hh, d_hh_p, d_hh_s, d_elec, d_sh, d_dhw], index=[\"hh\",\"hh_p\",\"hh_s\",\"res_elec\",\"res_sh\",\"res_dhw\"], columns=[DISTRICT])\n",
    "\n",
    "    TI_res = pd.concat([TI_res,d], axis=1)\n",
    "    district_res = district_res[0:0]\n",
    "\n",
    "TI_res[\"TI\"] = TI_res.sum(axis=1)\n",
    "\n",
    "path = str(DATA_DIRECTORY)+ \"/results/residential_demand.csv\"\n",
    "filepath = pathlib.Path(path)  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "TI_res.to_csv(filepath, sep=\";\", encoding='utf-8-sig', index_label='index')\n",
    "print(TI_res)\n",
    "TI_res = TI_res[0:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef8109-b46f-49a1-9abe-d8e0208fee51",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e1fb1-388b-4934-9a6b-fd5c15d971ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
